{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, explode, lit, size, desc\n",
    "from pyspark.sql.types import (\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/09 16:41:47 WARN Utils: Your hostname, workspace resolves to a loopback address: 127.0.1.1; using 11.11.1.73 instead (on interface eth0)\n",
      "23/05/09 16:41:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/terrabot/bk-imp/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/terrabot/.ivy2/cache\n",
      "The jars for the packages stored in: /home/terrabot/.ivy2/jars\n",
      "org.neo4j#neo4j-connector-apache-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1e78b784-b4a5-4879-8031-f9c475eebfd4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12;5.0.1_for_spark_3 in central\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12_common;5.0.1 in central\n",
      "\tfound org.neo4j.driver#neo4j-java-driver;4.4.11 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound org.apache.xbean#xbean-asm6-shaded;4.10 in central\n",
      "\tfound org.neo4j#neo4j-cypher-dsl;2020.1.4 in central\n",
      "\tfound org.apiguardian#apiguardian-api;1.1.0 in central\n",
      ":: resolution report :: resolve 239ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.xbean#xbean-asm6-shaded;4.10 from central in [default]\n",
      "\torg.apiguardian#apiguardian-api;1.1.0 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12;5.0.1_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12_common;5.0.1 from central in [default]\n",
      "\torg.neo4j#neo4j-cypher-dsl;2020.1.4 from central in [default]\n",
      "\torg.neo4j.driver#neo4j-java-driver;4.4.11 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   7   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1e78b784-b4a5-4879-8031-f9c475eebfd4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 7 already retrieved (0kB/6ms)\n",
      "23/05/09 16:41:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"bk-imp\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.neo4j:neo4j-connector-apache-spark_2.12:5.0.1_for_spark_3\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sampling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/09 16:42:12 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/05/09 16:42:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:42:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:42:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:43:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:43:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:43:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:43:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:43:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46628"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_schema = StructType(\n",
    "    [\n",
    "        StructField(\"asin\", StringType(), True),\n",
    "        StructField(\"reviewerID\", StringType(), True),\n",
    "        StructField(\"overall\", FloatType(), True),\n",
    "    ]\n",
    ")\n",
    "review_df = (\n",
    "    spark.read.schema(review_schema)\n",
    "    .json(\"../data/Automotive.json\")\n",
    "    .select(\n",
    "        col(\"asin\").alias(\"product_id\"),\n",
    "        col(\"reviewerID\").alias(\"reviewer_id\"),\n",
    "        col(\"overall\").alias(\"rating\"),\n",
    "    )\n",
    "    .dropDuplicates()\n",
    "    .repartition(8)\n",
    ")\n",
    "\n",
    "metadata_df = (\n",
    "    spark.read.json(\"../data/meta_Automotive.json\")\n",
    "    .select(\n",
    "        [\n",
    "            col(\"asin\").alias(\"product_id\"),\n",
    "            \"rank\",\n",
    "            \"category\",\n",
    "            \"description\",\n",
    "        ]\n",
    "    )\n",
    "    .dropDuplicates()\n",
    "    .repartition(8)\n",
    ")\n",
    "\n",
    "merged_df = review_df.join(metadata_df, [\"product_id\"])\n",
    "merged_df = (\n",
    "    merged_df.groupBy(\"product_id\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"reviewer_count\"),\n",
    "    )\n",
    "    .filter(\"reviewer_count >= 5\")\n",
    "    .limit(1000)\n",
    "    .join(merged_df, [\"product_id\"])\n",
    "    .select(\n",
    "        \"product_id\",\n",
    "        \"reviewer_id\",\n",
    "        \"rating\",\n",
    "        \"rank\",\n",
    "        \"category\",\n",
    "        \"description\",\n",
    "    )\n",
    "    .groupBy(\"reviewer_id\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"review_count\"),\n",
    "    )\n",
    "    .filter(\"review_count >= 2\")\n",
    "    .join(merged_df, [\"reviewer_id\"])\n",
    "    .select(\n",
    "        \"product_id\",\n",
    "        \"reviewer_id\",\n",
    "        \"rating\",\n",
    "        \"rank\",\n",
    "        \"category\",\n",
    "        \"description\",\n",
    "    )\n",
    ")\n",
    "\n",
    "merged_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/09 16:44:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:44:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:44:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:44:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:44:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:44:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:44:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:44:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 106:=============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------+--------------------+--------------------+--------------------+\n",
      "|product_id|   reviewer_id|rating|                rank|            category|         description|\n",
      "+----------+--------------+------+--------------------+--------------------+--------------------+\n",
      "|B0001CMUV4|A1646HQFB6TKQ3|   5.0|[\">#768 in Automo...|[Automotive, Exte...|[The StowAway hit...|\n",
      "|B00029WVJC|A3F7HEE8XEF335|   3.0|[\">#58,233 in Aut...|[Automotive, Repl...|[The Auxiliary Mi...|\n",
      "|B0002SR4Q8| ADSL9QVBI9GTU|   4.0|[\">#2,165 in Auto...|[Automotive, Tool...|[Removes all oil ...|\n",
      "+----------+--------------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merged_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/09 16:45:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:45:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:45:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:45:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:45:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:45:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:45:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:45:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:45:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merged_df.write.parquet(\"../data/sampled_data\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = merged_df.select([\"product_id\", \"reviewer_id\", \"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/09 16:46:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:46:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:46:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:46:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:47:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:47:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:47:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/05/09 16:47:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 253:>                                                        (0 + 4) / 8]\r"
     ]
    }
   ],
   "source": [
    "# Write nodes to Neo4j\n",
    "df.select(col(\"product_id\").alias(\"id\")).dropDuplicates().repartition(\n",
    "    8\n",
    ").write.format(\"org.neo4j.spark.DataSource\").option(\n",
    "    \"url\", \"bolt://localhost:7687\"\n",
    ").option(\n",
    "    \"node.keys\", \"id\"\n",
    ").option(\n",
    "    \"labels\", \":Product\"\n",
    ").mode(\n",
    "    \"overwrite\"\n",
    ").save()\n",
    "df.select(col(\"reviewer_id\").alias(\"id\")).dropDuplicates().repartition(\n",
    "    8\n",
    ").write.format(\"org.neo4j.spark.DataSource\").option(\n",
    "    \"url\", \"bolt://localhost:7687\"\n",
    ").option(\n",
    "    \"authentication.basic.username\", \"neo4j\"\n",
    ").option(\n",
    "    \"authentication.basic.password\", \"bitnami1\"\n",
    ").option(\n",
    "    \"node.keys\", \"id\"\n",
    ").option(\n",
    "    \"labels\", \":User\"\n",
    ").mode(\n",
    "    \"overwrite\"\n",
    ").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write relationships to Neo4j\n",
    "df.repartition(8).write.format(\"org.neo4j.spark.DataSource\").option(\n",
    "    \"url\", \"bolt://localhost:7687\"\n",
    ").option(\"relationship.save.strategy\", \"keys\").option(\n",
    "    \"relationship\", \"reviews\"\n",
    ").option(\n",
    "    \"relationship.properties\", \"rating\"\n",
    ").option(\n",
    "    \"relationship.source.labels\", \":User\"\n",
    ").option(\n",
    "    \"relationship.source.node.keys\", \"reviewer_id:id\"\n",
    ").option(\n",
    "    \"relationship.target.labels\", \":Product\"\n",
    ").option(\n",
    "    \"relationship.target.node.keys\", \"product_id:id\"\n",
    ").mode(\n",
    "    \"overwrite\"\n",
    ").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = merged_df.select(\n",
    "    [\"also_buy\", \"also_view\", \"product_id\"]\n",
    ").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Explode also_buy\n",
    "also_buy_df = df.select(\n",
    "    col(\"product_id\").alias(\"src_product_id\"),\n",
    "    explode(\"also_buy\").alias(\"dst_product_id\"),\n",
    "    lit(\"same_buyer\").alias(\"relationship\"),\n",
    ")\n",
    "\n",
    "# Explode also_view\n",
    "also_view_df = df.select(\n",
    "    col(\"product_id\").alias(\"src_product_id\"),\n",
    "    explode(\"also_view\").alias(\"dst_product_id\"),\n",
    "    lit(\"same_viewer\").alias(\"relationship\"),\n",
    ")\n",
    "\n",
    "# Union the two dataframes\n",
    "result_df = also_buy_df.union(also_view_df).dropDuplicates(\n",
    "    [\"src_product_id\", \"dst_product_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dataframe with distinct product IDs\n",
    "nodes_df = (\n",
    "    result_df.select(col(\"src_product_id\").alias(\"id\"))\n",
    "    .union(result_df.select(col(\"dst_product_id\").alias(\"id\")))\n",
    "    .distinct()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write nodes to Neo4j\n",
    "nodes_df.repartition(8).write.format(\"org.neo4j.spark.DataSource\").option(\n",
    "    \"url\", \"bolt://localhost:7687\"\n",
    ").option(\"node.keys\", \"id\").option(\"labels\", \":Product\").mode(\n",
    "    \"overwrite\"\n",
    ").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write relationships to Neo4j\n",
    "for relationship in [\"same_buyer\", \"same_viewer\"]:\n",
    "    relationships_df = result_df.filter(result_df.relationship == relationship)\n",
    "    relationships_df.repartition(8).write.format(\n",
    "        \"org.neo4j.spark.DataSource\"\n",
    "    ).option(\"url\", \"bolt://localhost:7687\").option(\n",
    "        \"relationship.save.strategy\", \"keys\"\n",
    "    ).option(\n",
    "        \"relationship\", relationship\n",
    "    ).option(\n",
    "        \"relationship.source.labels\", \":Product\"\n",
    "    ).option(\n",
    "        \"relationship.source.node.keys\", \"src_product_id:id\"\n",
    "    ).option(\n",
    "        \"relationship.target.labels\", \":Product\"\n",
    "    ).option(\n",
    "        \"relationship.target.node.keys\", \"dst_product_id:id\"\n",
    "    ).mode(\n",
    "        \"overwrite\"\n",
    "    ).save()\n",
    "    relationships_df.repartition(8).write.format(\n",
    "        \"org.neo4j.spark.DataSource\"\n",
    "    ).option(\"url\", \"bolt://localhost:7687\").option(\n",
    "        \"relationship.save.strategy\", \"keys\"\n",
    "    ).option(\n",
    "        \"relationship\", relationship\n",
    "    ).option(\n",
    "        \"relationship.source.labels\", \":Product\"\n",
    "    ).option(\n",
    "        \"relationship.source.node.keys\", \"dst_product_id:id\"\n",
    "    ).option(\n",
    "        \"relationship.target.labels\", \":Product\"\n",
    "    ).option(\n",
    "        \"relationship.target.node.keys\", \"src_product_id:id\"\n",
    "    ).mode(\n",
    "        \"overwrite\"\n",
    "    ).save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
