{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, explode, lit, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/06 05:53:29 WARN Utils: Your hostname, workspace resolves to a loopback address: 127.0.1.1; using 11.11.1.73 instead (on interface eth0)\n",
      "23/05/06 05:53:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/terrabot/bk-imp/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/terrabot/.ivy2/cache\n",
      "The jars for the packages stored in: /home/terrabot/.ivy2/jars\n",
      "org.neo4j#neo4j-connector-apache-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f0e20e41-a7da-4ba3-8e7e-ace7a2c04ece;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12;5.0.1_for_spark_3 in central\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12_common;5.0.1 in central\n",
      "\tfound org.neo4j.driver#neo4j-java-driver;4.4.11 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound org.apache.xbean#xbean-asm6-shaded;4.10 in central\n",
      "\tfound org.neo4j#neo4j-cypher-dsl;2020.1.4 in central\n",
      "\tfound org.apiguardian#apiguardian-api;1.1.0 in central\n",
      ":: resolution report :: resolve 242ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.xbean#xbean-asm6-shaded;4.10 from central in [default]\n",
      "\torg.apiguardian#apiguardian-api;1.1.0 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12;5.0.1_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12_common;5.0.1 from central in [default]\n",
      "\torg.neo4j#neo4j-cypher-dsl;2020.1.4 from central in [default]\n",
      "\torg.neo4j.driver#neo4j-java-driver;4.4.11 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   7   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f0e20e41-a7da-4ba3-8e7e-ace7a2c04ece\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 7 already retrieved (0kB/6ms)\n",
      "23/05/06 05:53:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"bk-imp\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.neo4j:neo4j-connector-apache-spark_2.12:5.0.1_for_spark_3\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sampling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "review_df = (\n",
    "    spark.read.json(\"../data/Digital_Music.json\")\n",
    "    .select(\n",
    "        col(\"asin\").alias(\"product_id\"),\n",
    "        col(\"reviewerID\").alias(\"reviewer_id\"),\n",
    "        col(\"overall\").alias(\"rating\"),\n",
    "    )\n",
    "    .dropDuplicates()\n",
    ")\n",
    "metadata_df = (\n",
    "    spark.read.json(\"../data/meta_Digital_Music.json\")\n",
    "    .select([\"also_buy\", \"also_view\", col(\"asin\").alias(\"product_id\")])\n",
    "    .filter((size(col(\"also_buy\")) >= 3) & (size(col(\"also_view\")) >= 3))\n",
    ")\n",
    "merged_df = review_df.join(metadata_df, [\"product_id\"]).sample(fraction=0.20)\n",
    "merged_df = (\n",
    "    merged_df.groupBy(\"reviewer_id\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"review_count\"),\n",
    "    )\n",
    "    .filter(\"review_count >= 3\")\n",
    "    .join(merged_df, [\"reviewer_id\"])\n",
    "    .select(\"product_id\", \"reviewer_id\", \"rating\", \"also_buy\", \"also_view\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------+--------------------+--------------------+\n",
      "|product_id|   reviewer_id|rating|            also_buy|           also_view|\n",
      "+----------+--------------+------+--------------------+--------------------+\n",
      "|B0057PSUZA|A2WQY1B8ZS7QRZ|   4.0|[B00MG4CVAU, B00N...|[B000063DFN, B00N...|\n",
      "|B000NP3HPW|A2WQY1B8ZS7QRZ|   4.0|[B07B64T2V9, B01N...|[B00DDVPXPK, B000...|\n",
      "|B00JKHYLKO|A2WQY1B8ZS7QRZ|   5.0|[B07JHJTPZ5, B00Y...|[B00YC26BLO, B00I...|\n",
      "+----------+--------------+------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merged_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = merged_df.select([\"product_id\", \"reviewer_id\", \"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write nodes to Neo4j\n",
    "df.select(col(\"product_id\").alias(\"id\")).dropDuplicates().write.format(\n",
    "    \"org.neo4j.spark.DataSource\"\n",
    ").option(\"url\", \"bolt://localhost:7687\").option(\"node.keys\", \"id\").option(\n",
    "    \"labels\", \":Product\"\n",
    ").mode(\n",
    "    \"overwrite\"\n",
    ").save()\n",
    "df.select(col(\"reviewer_id\").alias(\"id\")).dropDuplicates().write.format(\n",
    "    \"org.neo4j.spark.DataSource\"\n",
    ").option(\"url\", \"bolt://localhost:7687\").option(\n",
    "    \"authentication.basic.username\", \"neo4j\"\n",
    ").option(\n",
    "    \"authentication.basic.password\", \"bitnami1\"\n",
    ").option(\n",
    "    \"node.keys\", \"id\"\n",
    ").option(\n",
    "    \"labels\", \":User\"\n",
    ").mode(\n",
    "    \"overwrite\"\n",
    ").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write relationships to Neo4j\n",
    "df.write.format(\"org.neo4j.spark.DataSource\").option(\n",
    "    \"url\", \"bolt://localhost:7687\"\n",
    ").option(\"relationship.save.strategy\", \"keys\").option(\n",
    "    \"relationship\", \"reviews\"\n",
    ").option(\n",
    "    \"relationship.properties\", \"rating\"\n",
    ").option(\n",
    "    \"relationship.source.labels\", \":User\"\n",
    ").option(\n",
    "    \"relationship.source.node.keys\", \"reviewer_id:id\"\n",
    ").option(\n",
    "    \"relationship.target.labels\", \":Product\"\n",
    ").option(\n",
    "    \"relationship.target.node.keys\", \"product_id:id\"\n",
    ").mode(\n",
    "    \"overwrite\"\n",
    ").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = merged_df.select([\"also_buy\", \"also_view\", \"product_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Explode also_buy\n",
    "also_buy_df = df.select(\n",
    "    col(\"product_id\").alias(\"src_product_id\"),\n",
    "    explode(\"also_buy\").alias(\"dst_product_id\"),\n",
    "    lit(\"same_buyer\").alias(\"relationship\"),\n",
    ")\n",
    "\n",
    "# Explode also_view\n",
    "also_view_df = df.select(\n",
    "    col(\"product_id\").alias(\"src_product_id\"),\n",
    "    explode(\"also_view\").alias(\"dst_product_id\"),\n",
    "    lit(\"same_viewer\").alias(\"relationship\"),\n",
    ")\n",
    "\n",
    "# Union the two dataframes\n",
    "result_df = also_buy_df.union(also_view_df).dropDuplicates(\n",
    "    [\"src_product_id\", \"dst_product_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dataframe with distinct product IDs\n",
    "nodes_df = (\n",
    "    result_df.select(col(\"src_product_id\").alias(\"id\"))\n",
    "    .union(result_df.select(col(\"dst_product_id\").alias(\"id\")))\n",
    "    .distinct()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write nodes to Neo4j\n",
    "nodes_df.repartition(8).write.format(\"org.neo4j.spark.DataSource\").option(\n",
    "    \"url\", \"bolt://localhost:7687\"\n",
    ").option(\"node.keys\", \"id\").option(\"labels\", \":Product\").mode(\n",
    "    \"overwrite\"\n",
    ").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write relationships to Neo4j\n",
    "for relationship in [\"same_buyer\", \"same_viewer\"]:\n",
    "    relationships_df = result_df.filter(result_df.relationship == relationship)\n",
    "    relationships_df.repartition(8).write.format(\n",
    "        \"org.neo4j.spark.DataSource\"\n",
    "    ).option(\"url\", \"bolt://localhost:7687\").option(\n",
    "        \"relationship.save.strategy\", \"keys\"\n",
    "    ).option(\n",
    "        \"relationship\", relationship\n",
    "    ).option(\n",
    "        \"relationship.source.labels\", \":Product\"\n",
    "    ).option(\n",
    "        \"relationship.source.node.keys\", \"src_product_id:id\"\n",
    "    ).option(\n",
    "        \"relationship.target.labels\", \":Product\"\n",
    "    ).option(\n",
    "        \"relationship.target.node.keys\", \"dst_product_id:id\"\n",
    "    ).mode(\n",
    "        \"overwrite\"\n",
    "    ).save()\n",
    "    relationships_df.repartition(8).write.format(\n",
    "        \"org.neo4j.spark.DataSource\"\n",
    "    ).option(\"url\", \"bolt://localhost:7687\").option(\n",
    "        \"relationship.save.strategy\", \"keys\"\n",
    "    ).option(\n",
    "        \"relationship\", relationship\n",
    "    ).option(\n",
    "        \"relationship.source.labels\", \":Product\"\n",
    "    ).option(\n",
    "        \"relationship.source.node.keys\", \"dst_product_id:id\"\n",
    "    ).option(\n",
    "        \"relationship.target.labels\", \":Product\"\n",
    "    ).option(\n",
    "        \"relationship.target.node.keys\", \"src_product_id:id\"\n",
    "    ).mode(\n",
    "        \"overwrite\"\n",
    "    ).save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
