@misc{bai2022training,
  title         = {Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
  author        = {Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
  year          = {2022},
  eprint        = {2204.05862},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{chen2023teaching,
  title         = {Teaching Large Language Models to Self-Debug},
  author        = {Xinyun Chen and Maxwell Lin and Nathanael Schärli and Denny Zhou},
  year          = {2023},
  eprint        = {2304.05128},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{chollet2019measure,
  title         = {On the Measure of Intelligence},
  author        = {François Chollet},
  year          = {2019},
  eprint        = {1911.01547},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@misc{cobbe2021training,
  title         = {Training Verifiers to Solve Math Word Problems},
  author        = {Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
  year          = {2021},
  eprint        = {2110.14168},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{dong2022survey,
  title   = {A survey for in-context learning},
  author  = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal = {arXiv preprint arXiv:2301.00234},
  year    = {2022}
}
@misc{du2023improving,
  title         = {Improving Factuality and Reasoning in Language Models through Multiagent Debate},
  author        = {Yilun Du and Shuang Li and Antonio Torralba and Joshua B. Tenenbaum and Igor Mordatch},
  year          = {2023},
  eprint        = {2305.14325},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{dubois2024alpacafarm,
  title         = {AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback},
  author        = {Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
  year          = {2024},
  eprint        = {2305.14387},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{gao2023rarr,
  title         = {RARR: Researching and Revising What Language Models Say, Using Language Models},
  author        = {Luyu Gao and Zhuyun Dai and Panupong Pasupat and Anthony Chen and Arun Tejasvi Chaganty and Yicheng Fan and Vincent Y. Zhao and Ni Lao and Hongrae Lee and Da-Cheng Juan and Kelvin Guu},
  year          = {2023},
  eprint        = {2210.08726},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{gou2023critic,
  title         = {CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing},
  author        = {Zhibin Gou and Zhihong Shao and Yeyun Gong and Yelong Shen and Yujiu Yang and Nan Duan and Weizhu Chen},
  year          = {2023},
  eprint        = {2305.11738},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{he2022rethinking,
  title         = {Rethinking with Retrieval: Faithful Large Language Model Inference},
  author        = {Hangfeng He and Hongming Zhang and Dan Roth},
  year          = {2022},
  eprint        = {2301.00303},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{huang2022large,
  title         = {Large Language Models Can Self-Improve},
  author        = {Jiaxin Huang and Shixiang Shane Gu and Le Hou and Yuexin Wu and Xuezhi Wang and Hongkun Yu and Jiawei Han},
  year          = {2022},
  eprint        = {2210.11610},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{le2023codechain,
  title         = {CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules},
  author        = {Hung Le and Hailin Chen and Amrita Saha and Akash Gokul and Doyen Sahoo and Shafiq Joty},
  year          = {2023},
  eprint        = {2310.08992},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@misc{lu2022quark,
  title         = {Quark: Controllable Text Generation with Reinforced Unlearning},
  author        = {Ximing Lu and Sean Welleck and Jack Hessel and Liwei Jiang and Lianhui Qin and Peter West and Prithviraj Ammanabrolu and Yejin Choi},
  year          = {2022},
  eprint        = {2205.13636},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{olausson2023selfrepair,
  title         = {Is Self-Repair a Silver Bullet for Code Generation?},
  author        = {Theo X. Olausson and Jeevana Priya Inala and Chenglong Wang and Jianfeng Gao and Armando Solar-Lezama},
  year          = {2023},
  eprint        = {2306.09896},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{pan2023logiclm,
  title         = {Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning},
  author        = {Liangming Pan and Alon Albalak and Xinyi Wang and William Yang Wang},
  year          = {2023},
  eprint        = {2305.12295},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{peng2023impact,
  title         = {The Impact of AI on Developer Productivity: Evidence from GitHub Copilot},
  author        = {Sida Peng and Eirini Kalliamvakou and Peter Cihon and Mert Demirer},
  year          = {2023},
  eprint        = {2302.06590},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE}
}
@inproceedings{pmlr-v202-lai23b,
  title     = {{DS}-1000: A Natural and Reliable Benchmark for Data Science Code Generation},
  author    = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-Tau and Fried, Daniel and Wang, Sida and Yu, Tao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  pages     = {18319--18345},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume    = {202},
  series    = {Proceedings of Machine Learning Research},
  month     = {23--29 Jul},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v202/lai23b/lai23b.pdf},
  url       = {https://proceedings.mlr.press/v202/lai23b.html},
  abstract  = {We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as Numpy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) – across all Codex-002-predicted solutions that our evaluation accepts, only 1.8% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.}
}
@misc{wei2023chainofthought,
  title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author        = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
  year          = {2023},
  eprint        = {2201.11903},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{weng2023large,
  title         = {Large Language Models are Better Reasoners with Self-Verification},
  author        = {Yixuan Weng and Minjun Zhu and Fei Xia and Bin Li and Shizhu He and Shengping Liu and Bin Sun and Kang Liu and Jun Zhao},
  year          = {2023},
  eprint        = {2212.09561},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@misc{xie2023selfevaluation,
  title         = {Self-Evaluation Guided Beam Search for Reasoning},
  author        = {Yuxi Xie and Kenji Kawaguchi and Yiran Zhao and Xu Zhao and Min-Yen Kan and Junxian He and Qizhe Xie},
  year          = {2023},
  eprint        = {2305.00633},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{yao2023tree,
  title         = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  author        = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
  year          = {2023},
  eprint        = {2305.10601},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{zhang2023language,
  title         = {How Language Model Hallucinations Can Snowball},
  author        = {Muru Zhang and Ofir Press and William Merrill and Alisa Liu and Noah A. Smith},
  year          = {2023},
  eprint        = {2305.13534},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{zhang2023selfedit,
  title         = {Self-Edit: Fault-Aware Code Editor for Code Generation},
  author        = {Kechi Zhang and Zhuo Li and Jia Li and Ge Li and Zhi Jin},
  year          = {2023},
  eprint        = {2305.04087},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE}
}
