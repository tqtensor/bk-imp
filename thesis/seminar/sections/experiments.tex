\section{Experiments and Results}

\begin{frame}{Experiments}
    Three types of experiments were conducted, each involving a single pass through 1000 problems due to cost constraints.

    \begin{enumerate}
        \item \textbf{Full System:} The CoT-SelfEvolve model was evaluated alongside six different Language Learning Models (LLMs), including GPT-3.5-turbo, GPT-4, Claude 2.1, Claude 3, Mistral Large, and Mistral 8x7B.

        \item \textbf{Auto-CoT Generators:} The functionality of Auto-CoT generators was toggled on and off to examine their impact on the performance of CoT-SelfEvolve.

        \item \textbf{Large LLM as Teacher:} A large LLM, such as GPT-4, was used for CoT prompt generation, while GPT-3.5-turbo was used for code generation.
    \end{enumerate}
\end{frame}

\begin{frame}{Result: Full System}
    The CoT-SelfEvolve model, employing GPT-4 as the base LLM, outperformed the original SelfEvolve model in handling Pytorch, Sklearn, and Matplotlib questions, demonstrating a clear superiority.

    \begin{table}[H]
        \caption*{$\text{Pass@5}$ results on the DS-1000 dataset. (\%)}
        \resizebox{\textwidth}{!}{%
            \begin{tabular}{|l|l|c|c|c|c|c|c|c|}
                \hline
                                                         & LLM           & \multicolumn{1}{l|}{Scipy} & \multicolumn{1}{l|}{Pytorch} & \multicolumn{1}{l|}{Sklearn} & \multicolumn{1}{l|}{Matplotlib} & \multicolumn{1}{l|}{Pandas} & \multicolumn{1}{l|}{Numpy} & \multicolumn{1}{l|}{Tensorflow} \\ \hline
                \multirow{2}{*}{SelfEvolve}              & GPT-3.5       & 52.83                      & 64.71                        & 73.04                        & 78.06                           & N/A                         & N/A                        & N/A                             \\ \cline{2-9}
                                                         & GPT-4         & \textbf{58.49}             & 70.59                        & 70.43                        & 84.52                           & N/A                         & N/A                        & N/A                             \\ \hline
                \multirow{6}{*}{\textit{CoT-SelfEvolve}} & GPT-3.5       & 32.08                      & 72.06                        & 66.09                        & 32.26                           & 29.55                       & 17.73                      & 46.67                           \\ \cline{2-9}
                                                         & GPT-4         & 53.77                      & \textbf{89.71}               & \textbf{97.39}               & \textbf{85.16}                  & \textbf{92.78}              & \textbf{76.36}             & \textbf{84.44}                  \\ \cline{2-9}
                                                         & Claude 2.1    & 47.17                      & 83.82                        & 85.22                        & 83.23                           & 65.64                       & 61.82                      & 57.78                           \\ \cline{2-9}
                                                         & Claude 3      & 44.34                      & 76.47                        & 73.91                        & 85.81                           & 43.64                       & 31.36                      & 53.33                           \\ \cline{2-9}
                                                         & Mistral Large & 45.28                      & 80.88                        & 78.26                        & 52.26                           & 68.04                       & 43.64                      & 53.33                           \\ \cline{2-9}
                                                         & Mistral 8x7B  & 30.19                      & 57.35                        & 56.52                        & 69.68                           & 38.49                       & 35.91                      & 44.44                           \\ \hline
            \end{tabular}%
        }
    \end{table}
\end{frame}

\begin{frame}{Result: Auto-CoT Generators}
    The performance enhancement brought about by the Auto-CoT prompt generators is evident when compared to the non-CoT prompt version, with a relative gain of \textbf{16.39\%}. This finding highlights the significance of Auto-CoT prompt generators in augmenting the efficacy of the CoT-SelfEvolve model.

    \begin{table}[H]
        \caption*{$\text{Pass@5}$ results on the DS-1000 dataset w or w/o CoT prompts. (\%)}
        \centering
        \begin{tabular}{|c|c|c|c|}
            \hline
            Configuration & \begin{tabular}[c]{@{}c@{}}Initial\\ Code Generator\end{tabular} & \begin{tabular}[c]{@{}c@{}}Self-Correcting\\ Code Generator\end{tabular} & \multicolumn{1}{l|}{Overall} \\ \hline
            1             & GPT-3.5                                                          & GPT-3.5                                                                  & 30.52                        \\ \hline
            2             & GPT-3.5 + CoT                                                    & GPT-3.5                                                                  & 34.73                        \\ \hline
            3             & GPT-3.5                                                          & GPT-3.5 + CoT                                                            & 32.52                        \\ \hline
            4             & GPT-3.5 + CoT                                                    & GPT-3.5 + CoT                                                            & 35.51                        \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}{Result: Large LLM as Teacher}
    The data clearly shows that leveraging a larger LLM to steer the code generation process can significantly enhance the model's performance, as evidenced by a relative gain of \textbf{11.26\%}.

    \begin{table}[H]
        \caption*{$\text{Pass@5}$ results on the DS-1000 dataset with different LLM stacks. (\%)}
        \centering
        \resizebox{\columnwidth}{!}{%
            \begin{tabular}{|l|c|c|c|c|c|c|c|}
                \hline
                Configuration              & \multicolumn{1}{l|}{Scipy} & \multicolumn{1}{l|}{Pytorch} & \multicolumn{1}{l|}{Sklearn} & \multicolumn{1}{l|}{Matplotlib} & \multicolumn{1}{l|}{Pandas} & \multicolumn{1}{l|}{Numpy} & \multicolumn{1}{l|}{Tensorflow} \\ \hline
                GPT-3.5 CoT + GPT-3.5 Code & 32.08                      & 72.06                        & 66.09                        & 32.26                           & 29.55                       & 17.73                      & 46.67                           \\ \hline
                GPT-4 CoT + GPT-3.5 Code   & 37.74                      & 83.82                        & 73.04                        & 35.48                           & 31.62                       & 19.55                      & 53.33                           \\ \hline
            \end{tabular}%
        }
    \end{table}
\end{frame}
