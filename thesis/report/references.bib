@misc{abid2021persistent,
  title         = {Persistent Anti-Muslim Bias in Large Language Models},
  author        = {Abubakar Abid and Maheen Farooqi and James Zou},
  year          = {2021},
  eprint        = {2101.05783},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{akyurek2023rl4f,
  title   = {Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs},
  author  = {Aky{\"u}rek, Afra Feyza and Aky{\"u}rek, Ekin and Madaan, Aman and Kalyan, Ashwin and Clark, Peter and Wijaya, Derry and Tandon, Niket},
  journal = {arXiv preprint arXiv:2305.08844},
  year    = {2023}
}
@article{askell2021general,
  title   = {A general language assistant as a laboratory for alignment},
  author  = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal = {arXiv preprint arXiv:2112.00861},
  year    = {2021}
}
@article{baevski2018adaptive,
  title   = {Adaptive input representations for neural language modeling},
  author  = {Baevski, Alexei and Auli, Michael},
  journal = {arXiv preprint arXiv:1809.10853},
  year    = {2018}
}
@article{bahdanau2014neural,
  title   = {Neural machine translation by jointly learning to align and translate},
  author  = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal = {arXiv preprint arXiv:1409.0473},
  year    = {2014}
}
@article{bai2022constitutional,
  title   = {Constitutional ai: Harmlessness from ai feedback},
  author  = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal = {arXiv preprint arXiv:2212.08073},
  year    = {2022}
}
@article{bai2022training,
  title   = {Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author  = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal = {arXiv preprint arXiv:2204.05862},
  year    = {2022}
}
@article{brown2020language,
  title   = {Language models are few-shot learners},
  author  = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {1877--1901},
  year    = {2020}
}
@misc{chang2023learning,
  title         = {Learning to Generate Better Than Your LLM},
  author        = {Jonathan D. Chang and Kiante Brantley and Rajkumar Ramamurthy and Dipendra Misra and Wen Sun},
  year          = {2023},
  eprint        = {2306.11816},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{charalambous2023new,
  title         = {A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification},
  author        = {Yiannis Charalambous and Norbert Tihanyi and Ridhi Jain and Youcheng Sun and Mohamed Amine Ferrag and Lucas C. Cordeiro},
  year          = {2023},
  eprint        = {2305.14752},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE}
}
@article{chen2021evaluating,
  title   = {Evaluating large language models trained on code},
  author  = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal = {arXiv preprint arXiv:2107.03374},
  year    = {2021}
}
@misc{chen2022codet,
  title         = {CodeT: Code Generation with Generated Tests},
  author        = {Bei Chen and Fengji Zhang and Anh Nguyen and Daoguang Zan and Zeqi Lin and Jian-Guang Lou and Weizhu Chen},
  year          = {2022},
  eprint        = {2207.10397},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{chen2023improving,
  title         = {Improving Code Generation by Training with Natural Language Feedback},
  author        = {Angelica Chen and Jérémy Scheurer and Tomasz Korbak and Jon Ander Campos and Jun Shern Chan and Samuel R. Bowman and Kyunghyun Cho and Ethan Perez},
  year          = {2023},
  eprint        = {2303.16749},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE}
}
@misc{chen2023teaching,
  title         = {Teaching Large Language Models to Self-Debug},
  author        = {Xinyun Chen and Maxwell Lin and Nathanael Schärli and Denny Zhou},
  year          = {2023},
  eprint        = {2304.05128},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{chern2023factool,
  title         = {FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios},
  author        = {I-Chun Chern and Steffi Chern and Shiqi Chen and Weizhe Yuan and Kehua Feng and Chunting Zhou and Junxian He and Graham Neubig and Pengfei Liu},
  year          = {2023},
  eprint        = {2307.13528},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{child2019generating,
  title   = {Generating long sequences with sparse transformers},
  author  = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal = {arXiv preprint arXiv:1904.10509},
  year    = {2019}
}
@article{chowdhery2023palm,
  title   = {Palm: Scaling language modeling with pathways},
  author  = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal = {Journal of Machine Learning Research},
  volume  = {24},
  number  = {240},
  pages   = {1--113},
  year    = {2023}
}
@article{chung2022scaling,
  title   = {Scaling instruction-finetuned language models},
  author  = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal = {arXiv preprint arXiv:2210.11416},
  year    = {2022}
}
@misc{clark2021thats,
  title         = {All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text},
  author        = {Elizabeth Clark and Tal August and Sofia Serrano and Nikita Haduong and Suchin Gururangan and Noah A. Smith},
  year          = {2021},
  eprint        = {2107.00061},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{cobbe2021training,
  title         = {Training Verifiers to Solve Math Word Problems},
  author        = {Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
  year          = {2021},
  eprint        = {2110.14168},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{cohen2023lm,
  title         = {LM vs LM: Detecting Factual Errors via Cross Examination},
  author        = {Roi Cohen and May Hamri and Mor Geva and Amir Globerson},
  year          = {2023},
  eprint        = {2305.13281},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{creswell2022faithful,
  title         = {Faithful Reasoning Using Large Language Models},
  author        = {Antonia Creswell and Murray Shanahan},
  year          = {2022},
  eprint        = {2208.14271},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@article{dao2022flashattention,
  title   = {Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author  = {Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {16344--16359},
  year    = {2022}
}
@misc{dathathri2020plug,
  title         = {Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
  author        = {Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},
  year          = {2020},
  eprint        = {1912.02164},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{dauphin2017language,
  title        = {Language modeling with gated convolutional networks},
  author       = {Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle    = {International conference on machine learning},
  pages        = {933--941},
  year         = {2017},
  organization = {PMLR}
}
@misc{devlin2019bert,
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year          = {2019},
  eprint        = {1810.04805},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{dong2022survey,
  title   = {A survey for in-context learning},
  author  = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal = {arXiv preprint arXiv:2301.00234},
  year    = {2022}
}
@misc{du2023improving,
  title         = {Improving Factuality and Reasoning in Language Models through Multiagent Debate},
  author        = {Yilun Du and Shuang Li and Antonio Torralba and Joshua B. Tenenbaum and Igor Mordatch},
  year          = {2023},
  eprint        = {2305.14325},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{dubois2024alpacafarm,
  title         = {AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback},
  author        = {Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
  year          = {2024},
  eprint        = {2305.14387},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{eriksen1972some,
  title     = {Some characteristics of selective attention in visual perception determined by vocal reaction time},
  author    = {Eriksen, Charles W and Hoffman, James E},
  journal   = {Perception \& Psychophysics},
  volume    = {11},
  number    = {2},
  pages     = {169--171},
  year      = {1972},
  publisher = {Springer}
}
@misc{fernandes2023bridging,
  title         = {Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation},
  author        = {Patrick Fernandes and Aman Madaan and Emmy Liu and António Farinhas and Pedro Henrique Martins and Amanda Bertsch and José G. C. de Souza and Shuyan Zhou and Tongshuang Wu and Graham Neubig and André F. T. Martins},
  year          = {2023},
  eprint        = {2305.00955},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{first2023baldur,
  title         = {Baldur: Whole-Proof Generation and Repair with Large Language Models},
  author        = {Emily First and Markus N. Rabe and Talia Ringer and Yuriy Brun},
  year          = {2023},
  eprint        = {2303.04910},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{freitag-etal-2022-high,
  title     = {High Quality Rather than High Model Probability: Minimum {B}ayes Risk Decoding with Neural Metrics},
  author    = {Freitag, Markus  and
               Grangier, David  and
               Tan, Qijun  and
               Liang, Bowen},
  editor    = {Roark, Brian  and
               Nenkova, Ani},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {10},
  year      = {2022},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/2022.tacl-1.47},
  doi       = {10.1162/tacl_a_00491},
  pages     = {811--825},
  abstract  = {In Neural Machine Translation, it is typically assumed that the sentence with the highest estimated probability should also be the translation with the highest quality as measured by humans. In this work, we question this assumption and show that model estimates and translation quality only vaguely correlate. We apply Minimum Bayes Risk (MBR) decoding on unbiased samples to optimize diverse automated metrics of translation quality as an alternative inference strategy to beam search. Instead of targeting the hypotheses with the highest model probability, MBR decoding extracts the hypotheses with the highest estimated quality. Our experiments show that the combination of a neural translation model with a neural reference-based metric, Bleurt, results in significant improvement in human evaluations. This improvement is obtained with translations different from classical beam-search output: These translations have much lower model likelihood and are less favored by surface metrics like Bleu.}
}
@misc{fu2023improving,
  title         = {Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback},
  author        = {Yao Fu and Hao Peng and Tushar Khot and Mirella Lapata},
  year          = {2023},
  eprint        = {2305.10142},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{ganguli2023capacity,
  title   = {The capacity for moral self-correction in large language models},
  author  = {Ganguli, Deep and Askell, Amanda and Schiefer, Nicholas and Liao, Thomas I and Luko{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and Olsson, Catherine and Hernandez, Danny and others},
  journal = {arXiv preprint arXiv:2302.07459},
  year    = {2023}
}
@misc{gao2023continually,
  title         = {Continually Improving Extractive QA via Human Feedback},
  author        = {Ge Gao and Hung-Ting Chen and Yoav Artzi and Eunsol Choi},
  year          = {2023},
  eprint        = {2305.12473},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{gao2023rarr,
  title         = {RARR: Researching and Revising What Language Models Say, Using Language Models},
  author        = {Luyu Gao and Zhuyun Dai and Panupong Pasupat and Anthony Chen and Arun Tejasvi Chaganty and Yicheng Fan and Vincent Y. Zhao and Ni Lao and Hongrae Lee and Da-Cheng Juan and Kelvin Guu},
  year          = {2023},
  eprint        = {2210.08726},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{gero2023selfverification,
  title         = {Self-Verification Improves Few-Shot Clinical Information Extraction},
  author        = {Zelalem Gero and Chandan Singh and Hao Cheng and Tristan Naumann and Michel Galley and Jianfeng Gao and Hoifung Poon},
  year          = {2023},
  eprint        = {2306.00024},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{glaese2022improving,
  title   = {Improving alignment of dialogue agents via targeted human judgements},
  author  = {Glaese, Amelia and McAleese, Nat and Tr{\k{e}}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},
  journal = {arXiv preprint arXiv:2209.14375},
  year    = {2022}
}
@misc{goldstein2023generative,
  title         = {Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations},
  author        = {Josh A. Goldstein and Girish Sastry and Micah Musser and Renee DiResta and Matthew Gentzel and Katerina Sedova},
  year          = {2023},
  eprint        = {2301.04246},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CY}
}
@misc{golovneva2023roscoe,
  title         = {ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning},
  author        = {Olga Golovneva and Moya Chen and Spencer Poff and Martin Corredor and Luke Zettlemoyer and Maryam Fazel-Zarandi and Asli Celikyilmaz},
  year          = {2023},
  eprint        = {2212.07919},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{gou2023critic,
  title         = {CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing},
  author        = {Zhibin Gou and Zhihong Shao and Yeyun Gong and Yelong Shen and Yujiu Yang and Nan Duan and Weizhu Chen},
  year          = {2023},
  eprint        = {2305.11738},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{gulcehre2023reinforced,
  title         = {Reinforced Self-Training (ReST) for Language Modeling},
  author        = {Caglar Gulcehre and Tom Le Paine and Srivatsan Srinivasan and Ksenia Konyushkova and Lotte Weerts and Abhishek Sharma and Aditya Siddhant and Alex Ahern and Miaosen Wang and Chenjie Gu and Wolfgang Macherey and Arnaud Doucet and Orhan Firat and Nando de Freitas},
  year          = {2023},
  eprint        = {2308.08998},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{hao2023reasoning,
  title         = {Reasoning with Language Model is Planning with World Model},
  author        = {Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and Daisy Zhe Wang and Zhiting Hu},
  year          = {2023},
  eprint        = {2305.14992},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{he2021deberta,
  title         = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  author        = {Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
  year          = {2021},
  eprint        = {2006.03654},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{he2022rethinking,
  title         = {Rethinking with Retrieval: Faithful Large Language Model Inference},
  author        = {Hangfeng He and Hongming Zhang and Dan Roth},
  year          = {2022},
  eprint        = {2301.00303},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{hendrycks2016gaussian,
  title   = {Gaussian error linear units (gelus)},
  author  = {Hendrycks, Dan and Gimpel, Kevin},
  journal = {arXiv preprint arXiv:1606.08415},
  year    = {2016}
}
@misc{hendrycks2021measuring,
  title         = {Measuring Coding Challenge Competence With APPS},
  author        = {Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Song and Jacob Steinhardt},
  year          = {2021},
  eprint        = {2105.09938},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE}
}
@misc{hernandez2021scaling,
  title         = {Scaling Laws for Transfer},
  author        = {Danny Hernandez and Jared Kaplan and Tom Henighan and Sam McCandlish},
  year          = {2021},
  eprint        = {2102.01293},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{hornik1989multilayer,
  title     = {Multilayer feedforward networks are universal approximators},
  author    = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal   = {Neural networks},
  volume    = {2},
  number    = {5},
  pages     = {359--366},
  year      = {1989},
  publisher = {Elsevier}
}
@misc{huang2022large,
  title         = {Large Language Models Can Self-Improve},
  author        = {Jiaxin Huang and Shixiang Shane Gu and Le Hou and Yuexin Wu and Xuezhi Wang and Hongkun Yu and Jiawei Han},
  year          = {2022},
  eprint        = {2210.11610},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{huang2022towards,
  title   = {Towards reasoning in large language models: A survey},
  author  = {Huang, Jie and Chang, Kevin Chen-Chuan},
  journal = {arXiv preprint arXiv:2212.10403},
  year    = {2022}
}
@article{iyer2022opt,
  title   = {Opt-iml: Scaling language model instruction meta learning through the lens of generalization},
  author  = {Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and others},
  journal = {arXiv preprint arXiv:2212.12017},
  year    = {2022}
}
@misc{jang2017categorical,
  title         = {Categorical Reparameterization with Gumbel-Softmax},
  author        = {Eric Jang and Shixiang Gu and Ben Poole},
  year          = {2017},
  eprint        = {1611.01144},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}
@article{unanue2021berttune,
  title   = {BERTTune: Fine-tuning neural machine translation with BERTScore},
  author  = {Unanue, Inigo Jauregi and Parnell, Jacob and Piccardi, Massimo},
  journal = {arXiv preprint arXiv:2106.02208},
  year    = {2021}
}
@inproceedings{jung-etal-2022-maieutic,
  title     = {Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations},
  author    = {Jung, Jaehun  and
               Qin, Lianhui  and
               Welleck, Sean  and
               Brahman, Faeze  and
               Bhagavatula, Chandra  and
               Le Bras, Ronan  and
               Choi, Yejin},
  editor    = {Goldberg, Yoav  and
               Kozareva, Zornitsa  and
               Zhang, Yue},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  month     = dec,
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.emnlp-main.82},
  doi       = {10.18653/v1/2022.emnlp-main.82},
  pages     = {1266--1279},
  abstract  = {Pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which themselves are often noisy and inconsistent. In this work, we develop Maieutic Prompting, which aims to infer a correct answer to a question even from the unreliable generations of LM. Maieutic Prompting induces a tree of explanations abductively (e.g. X is true, because ...) and recursively, then frames the inference as a satisfiability problem over these explanations and their logical relations. We test Maieutic Prompting for true/false QA on three challenging benchmarks that require complex commonsense reasoning. Maieutic Prompting achieves up to 20{\%} better accuracy than state-of-the-art prompting methods, and as a fully unsupervised approach, performs competitively with supervised models. We also show that Maieutic Prompting improves robustness in inference while providing interpretable rationales.}
}
@misc{kaplan2020scaling,
  title         = {Scaling Laws for Neural Language Models},
  author        = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  year          = {2020},
  eprint        = {2001.08361},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{kenton2021alignment,
  title         = {Alignment of Language Agents},
  author        = {Zachary Kenton and Tom Everitt and Laura Weidinger and Iason Gabriel and Vladimir Mikulik and Geoffrey Irving},
  year          = {2021},
  eprint        = {2103.14659},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@misc{keskar2019ctrl,
  title         = {CTRL: A Conditional Transformer Language Model for Controllable Generation},
  author        = {Nitish Shirish Keskar and Bryan McCann and Lav R. Varshney and Caiming Xiong and Richard Socher},
  year          = {2019},
  eprint        = {1909.05858},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{khalifa2023grace,
  title         = {GRACE: Discriminator-Guided Chain-of-Thought Reasoning},
  author        = {Muhammad Khalifa and Lajanugen Logeswaran and Moontae Lee and Honglak Lee and Lu Wang},
  year          = {2023},
  eprint        = {2305.14934},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{khlaaf2022hazard,
  title         = {A Hazard Analysis Framework for Code Synthesis Large Language Models},
  author        = {Heidy Khlaaf and Pamela Mishkin and Joshua Achiam and Gretchen Krueger and Miles Brundage},
  year          = {2022},
  eprint        = {2207.14157},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE}
}
@article{kim2023cot,
  title   = {The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning},
  author  = {Kim, Seungone and Joo, Se June and Kim, Doyoung and Jang, Joel and Ye, Seonghyeon and Shin, Jamin and Seo, Minjoon},
  journal = {arXiv preprint arXiv:2305.14045},
  year    = {2023}
}
@misc{korbak2023pretraining,
  title         = {Pretraining Language Models with Human Preferences},
  author        = {Tomasz Korbak and Kejian Shi and Angelica Chen and Rasika Bhalerao and Christopher L. Buckley and Jason Phang and Samuel R. Bowman and Ethan Perez},
  year          = {2023},
  eprint        = {2302.08582},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{krueger2016zoneout,
  title   = {Zoneout: Regularizing rnns by randomly preserving hidden activations},
  author  = {Krueger, David and Maharaj, Tegan and Kram{\'a}r, J{\'a}nos and Pezeshki, Mohammad and Ballas, Nicolas and Ke, Nan Rosemary and Goyal, Anirudh and Bengio, Yoshua and Courville, Aaron and Pal, Chris},
  journal = {arXiv preprint arXiv:1606.01305},
  year    = {2016}
}
@article{kudo2018subword,
  title   = {Subword regularization: Improving neural network translation models with multiple subword candidates},
  author  = {Kudo, Taku},
  journal = {arXiv preprint arXiv:1804.10959},
  year    = {2018}
}
@misc{le2022coderl,
  title         = {CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning},
  author        = {Hung Le and Yue Wang and Akhilesh Deepak Gotmare and Silvio Savarese and Steven C. H. Hoi},
  year          = {2022},
  eprint        = {2207.01780},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{le2023codechain,
  title         = {CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules},
  author        = {Hung Le and Hailin Chen and Amrita Saha and Akash Gokul and Doyen Sahoo and Shafiq Joty},
  year          = {2023},
  eprint        = {2310.08992},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@article{Li_2022,
  title     = {Competition-level code generation with AlphaCode},
  volume    = {378},
  issn      = {1095-9203},
  url       = {http://dx.doi.org/10.1126/science.abq1158},
  doi       = {10.1126/science.abq1158},
  number    = {6624},
  journal   = {Science},
  publisher = {American Association for the Advancement of Science (AAAS)},
  author    = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, Rémi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and Hubert, Thomas and Choy, Peter and de Masson d’Autume, Cyprien and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Sutherland Robson, Esme and Kohli, Pushmeet and de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
  year      = {2022},
  month     = dec,
  pages     = {1092–1097}
}
@article{li2019deep,
  title   = {Deep reinforcement learning with distributional semantic rewards for abstractive summarization},
  author  = {Li, Siyao and Lei, Deren and Qin, Pengda and Wang, William Yang},
  journal = {arXiv preprint arXiv:1909.00141},
  year    = {2019}
}
@inproceedings{li-etal-2023-making,
  title     = {Making Language Models Better Reasoners with Step-Aware Verifier},
  author    = {Li, Yifei  and
               Lin, Zeqi  and
               Zhang, Shizhuo  and
               Fu, Qiang  and
               Chen, Bei  and
               Lou, Jian-Guang  and
               Chen, Weizhu},
  editor    = {Rogers, Anna  and
               Boyd-Graber, Jordan  and
               Okazaki, Naoaki},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.acl-long.291},
  doi       = {10.18653/v1/2023.acl-long.291},
  pages     = {5315--5333},
  abstract  = {Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9{\%} to 58.1{\%} in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4{\%} to 83.2{\%}).}
}
@misc{li2022diffusionlm,
  title         = {Diffusion-LM Improves Controllable Text Generation},
  author        = {Xiang Lisa Li and John Thickstun and Ishaan Gulrajani and Percy Liang and Tatsunori B. Hashimoto},
  year          = {2022},
  eprint        = {2205.14217},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{li2023halueval,
  title         = {HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models},
  author        = {Junyi Li and Xiaoxue Cheng and Wayne Xin Zhao and Jian-Yun Nie and Ji-Rong Wen},
  year          = {2023},
  eprint        = {2305.11747},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{li2023prd,
  title         = {PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations},
  author        = {Ruosen Li and Teerth Patel and Xinya Du},
  year          = {2023},
  eprint        = {2307.02762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{lightman2023lets,
  title         = {Let's Verify Step by Step},
  author        = {Hunter Lightman and Vineet Kosaraju and Yura Burda and Harri Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
  year          = {2023},
  eprint        = {2305.20050},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{liu2021simcls,
  title   = {SimCLS: A simple framework for contrastive learning of abstractive summarization},
  author  = {Liu, Yixin and Liu, Pengfei},
  journal = {arXiv preprint arXiv:2106.01890},
  year    = {2021}
}
@article{liu2018generating,
  title   = {Generating wikipedia by summarizing long sequences},
  author  = {Liu, Peter J and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
  journal = {arXiv preprint arXiv:1801.10198},
  year    = {2018}
}
@misc{liu2023chain,
  title         = {Chain of Hindsight Aligns Language Models with Feedback},
  author        = {Hao Liu and Carmelo Sferrazza and Pieter Abbeel},
  year          = {2023},
  eprint        = {2302.02676},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{liu2023zero,
  title   = {From zero to hero: Examining the power of symbolic tasks in instruction tuning},
  author  = {Liu, Qian and Zhou, Fan and Jiang, Zhengbao and Dou, Longxu and Lin, Min},
  journal = {arXiv preprint arXiv:2304.07995},
  year    = {2023}
}
@misc{lu2022quark,
  title         = {Quark: Controllable Text Generation with Reinforced Unlearning},
  author        = {Ximing Lu and Sean Welleck and Jack Hessel and Liwei Jiang and Lianhui Qin and Peter West and Prithviraj Ammanabrolu and Yejin Choi},
  year          = {2022},
  eprint        = {2205.13636},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{luo2023wizardcoder,
  title         = {WizardCoder: Empowering Code Large Language Models with Evol-Instruct},
  author        = {Ziyang Luo and Can Xu and Pu Zhao and Qingfeng Sun and Xiubo Geng and Wenxiang Hu and Chongyang Tao and Jing Ma and Qingwei Lin and Daxin Jiang},
  year          = {2023},
  eprint        = {2306.08568},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{lyu2023faithful,
  title         = {Faithful Chain-of-Thought Reasoning},
  author        = {Qing Lyu and Shreya Havaldar and Adam Stein and Li Zhang and Delip Rao and Eric Wong and Marianna Apidianaki and Chris Callison-Burch},
  year          = {2023},
  eprint        = {2301.13379},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{madaan2022memory,
  title   = {Memory-assisted prompt editing to improve gpt-3 after deployment},
  author  = {Madaan, Aman and Tandon, Niket and Clark, Peter and Yang, Yiming},
  journal = {arXiv preprint arXiv:2201.06009},
  year    = {2022}
}
@misc{madaan2022language,
  title         = {Language Models of Code are Few-Shot Commonsense Learners},
  author        = {Aman Madaan and Shuyan Zhou and Uri Alon and Yiming Yang and Graham Neubig},
  year          = {2022},
  eprint        = {2210.07128},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{madaan2024self,
  title   = {Self-refine: Iterative refinement with self-feedback},
  author  = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {36},
  year    = {2024}
}
@misc{mehrabi2023flirt,
  title         = {FLIRT: Feedback Loop In-context Red Teaming},
  author        = {Ninareh Mehrabi and Palash Goyal and Christophe Dupuy and Qian Hu and Shalini Ghosh and Richard Zemel and Kai-Wei Chang and Aram Galstyan and Rahul Gupta},
  year          = {2023},
  eprint        = {2308.04265},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@misc{mialon2023augmented,
  title         = {Augmented Language Models: a Survey},
  author        = {Grégoire Mialon and Roberto Dessì and Maria Lomeli and Christoforos Nalmpantis and Ram Pasunuru and Roberta Raileanu and Baptiste Rozière and Timo Schick and Jane Dwivedi-Yu and Asli Celikyilmaz and Edouard Grave and Yann LeCun and Thomas Scialom},
  year          = {2023},
  eprint        = {2302.07842},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{mielke2021between,
  title   = {Between words and characters: a brief history of open-vocabulary modeling and tokenization in nlp},
  author  = {Mielke, Sabrina J and Alyafeai, Zaid and Salesky, Elizabeth and Raffel, Colin and Dey, Manan and Gall{\'e}, Matthias and Raja, Arun and Si, Chenglei and Lee, Wilson Y and Sagot, Beno{\^\i}t and others},
  journal = {arXiv preprint arXiv:2112.10508},
  year    = {2021}
}
@inproceedings{nair2010rectified,
  title     = {Rectified linear units improve restricted boltzmann machines},
  author    = {Nair, Vinod and Hinton, Geoffrey E},
  booktitle = {Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages     = {807--814},
  year      = {2010}
}
@misc{ni2023lever,
  title         = {LEVER: Learning to Verify Language-to-Code Generation with Execution},
  author        = {Ansong Ni and Srini Iyer and Dragomir Radev and Ves Stoyanov and Wen-tau Yih and Sida I. Wang and Xi Victoria Lin},
  year          = {2023},
  eprint        = {2302.08468},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{olausson2023selfrepair,
  title         = {Is Self-Repair a Silver Bullet for Code Generation?},
  author        = {Theo X. Olausson and Jeevana Priya Inala and Chenglong Wang and Jianfeng Gao and Armando Solar-Lezama},
  year          = {2023},
  eprint        = {2306.09896},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{achiam2023gpt,
  title   = {Gpt-4 technical report},
  author  = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal = {arXiv preprint arXiv:2303.08774},
  year    = {2023}
}
@article{ouyang2022training,
  title   = {Training language models to follow instructions with human feedback},
  author  = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal = {Advances in neural information processing systems},
  volume  = {35},
  pages   = {27730--27744},
  year    = {2022}
}
@misc{pan2023logiclm,
  title         = {Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning},
  author        = {Liangming Pan and Alon Albalak and Xinyi Wang and William Yang Wang},
  year          = {2023},
  eprint        = {2305.12295},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{paul2023refiner,
  title         = {REFINER: Reasoning Feedback on Intermediate Representations},
  author        = {Debjit Paul and Mete Ismayilzada and Maxime Peyrard and Beatriz Borges and Antoine Bosselut and Robert West and Boi Faltings},
  year          = {2023},
  eprint        = {2304.01904},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{peng2023check,
  title         = {Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback},
  author        = {Baolin Peng and Michel Galley and Pengcheng He and Hao Cheng and Yujia Xie and Yu Hu and Qiuyuan Huang and Lars Liden and Zhou Yu and Weizhu Chen and Jianfeng Gao},
  year          = {2023},
  eprint        = {2302.12813},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{peng2023impact,
  title         = {The Impact of AI on Developer Productivity: Evidence from GitHub Copilot},
  author        = {Sida Peng and Eirini Kalliamvakou and Peter Cihon and Mert Demirer},
  year          = {2023},
  eprint        = {2302.06590},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE}
}
@inproceedings{lai2023ds,
  title        = {DS-1000: A natural and reliable benchmark for data science code generation},
  author       = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
  booktitle    = {International Conference on Machine Learning},
  pages        = {18319--18345},
  year         = {2023},
  organization = {PMLR}
}
@article{press2021train,
  title   = {Train short, test long: Attention with linear biases enables input length extrapolation},
  author  = {Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal = {arXiv preprint arXiv:2108.12409},
  year    = {2021}
}
@article{radford2019language,
  title   = {Language models are unsupervised multitask learners},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  volume  = {1},
  number  = {8},
  pages   = {9},
  year    = {2019}
}
@article{raffel2020exploring,
  title     = {Exploring the limits of transfer learning with a unified text-to-text transformer},
  author    = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal   = {The Journal of Machine Learning Research},
  volume    = {21},
  number    = {1},
  pages     = {5485--5551},
  year      = {2020},
  publisher = {JMLRORG}
}
@misc{ribeiro2023street,
  title         = {STREET: A Multi-Task Structured Reasoning and Explanation Benchmark},
  author        = {Danilo Ribeiro and Shen Wang and Xiaofei Ma and Henry Zhu and Rui Dong and Deguang Kong and Juliette Burger and Anjelica Ramos and William Wang and Zhiheng Huang and George Karypis and Bing Xiang and Dan Roth},
  year          = {2023},
  eprint        = {2302.06729},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{saravia2022prompt,
  title  = {Prompt Engineering Guide},
  author = {Saravia, Elvis},
  year   = {2022}
}
@misc{scheurer2023training,
  title         = {Training Language Models with Language Feedback at Scale},
  author        = {Jérémy Scheurer and Jon Ander Campos and Tomasz Korbak and Jun Shern Chan and Angelica Chen and Kyunghyun Cho and Ethan Perez},
  year          = {2023},
  eprint        = {2303.16755},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{schick2022peer,
  title         = {PEER: A Collaborative Language Model},
  author        = {Timo Schick and Jane Dwivedi-Yu and Zhengbao Jiang and Fabio Petroni and Patrick Lewis and Gautier Izacard and Qingfei You and Christoforos Nalmpantis and Edouard Grave and Sebastian Riedel},
  year          = {2022},
  eprint        = {2208.11663},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{schick2023toolformer,
  title         = {Toolformer: Language Models Can Teach Themselves to Use Tools},
  author        = {Timo Schick and Jane Dwivedi-Yu and Roberto Dessì and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
  year          = {2023},
  eprint        = {2302.04761},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{schramowski2022large,
  title         = {Large Pre-trained Language Models Contain Human-like Biases of What is Right and Wrong to Do},
  author        = {Patrick Schramowski and Cigdem Turan and Nico Andersen and Constantin A. Rothkopf and Kristian Kersting},
  year          = {2022},
  eprint        = {2103.11790},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{schulman2017proximal,
  title         = {Proximal Policy Optimization Algorithms},
  author        = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  year          = {2017},
  eprint        = {1707.06347},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@inproceedings{schuster2012japanese,
  title        = {Japanese and korean voice search},
  author       = {Schuster, Mike and Nakajima, Kaisuke},
  booktitle    = {2012 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages        = {5149--5152},
  year         = {2012},
  organization = {IEEE}
}
@misc{selfee2023,
  author       = {Ye, Seonghyeon and Jo, Yongrae and Kim, Doyoung and Kim, Sungdong and Hwang, Hyeonbin and Seo, Minjoon},
  title        = {SelFee: Iterative Self-Revising LLM Empowered by Self-Feedback Generation},
  url          = {https://kaistai.github.io/SelFee/},
  month        = {May},
  year         = {2023},
  howpublished = {Blog post}
}
@article{sellam2020bleurt,
  title   = {BLEURT: Learning robust metrics for text generation},
  author  = {Sellam, Thibault and Das, Dipanjan and Parikh, Ankur P},
  journal = {arXiv preprint arXiv:2004.04696},
  year    = {2020}
}
@article{sennrich2015neural,
  title   = {Neural machine translation of rare words with subword units},
  author  = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal = {arXiv preprint arXiv:1508.07909},
  year    = {2015}
}
@article{shazeer2020glu,
  title   = {Glu variants improve transformer},
  author  = {Shazeer, Noam},
  journal = {arXiv preprint arXiv:2002.05202},
  year    = {2020}
}
@article{shen2015minimum,
  title   = {Minimum risk training for neural machine translation},
  author  = {Shen, Shiqi and Cheng, Yong and He, Zhongjun and He, Wei and Wu, Hua and Sun, Maosong and Liu, Yang},
  journal = {arXiv preprint arXiv:1512.02433},
  year    = {2015}
}
@misc{shinn2023reflexion,
  title         = {Reflexion: Language Agents with Verbal Reinforcement Learning},
  author        = {Noah Shinn and Federico Cassano and Edward Berman and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
  year          = {2023},
  eprint        = {2303.11366},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@article{shleifer2021normformer,
  title   = {Normformer: Improved transformer pretraining with extra normalization},
  author  = {Shleifer, Sam and Weston, Jason and Ott, Myle},
  journal = {arXiv preprint arXiv:2110.09456},
  year    = {2021}
}
@misc{singla2015learning,
  title         = {Learning to Hire Teams},
  author        = {Adish Singla and Eric Horvitz and Pushmeet Kohli and Andreas Krause},
  year          = {2015},
  eprint        = {1508.02823},
  archiveprefix = {arXiv},
  primaryclass  = {cs.HC}
}
@misc{solaiman2019release,
  title         = {Release Strategies and the Social Impacts of Language Models},
  author        = {Irene Solaiman and Miles Brundage and Jack Clark and Amanda Askell and Ariel Herbert-Voss and Jeff Wu and Alec Radford and Gretchen Krueger and Jong Wook Kim and Sarah Kreps and Miles McCain and Alex Newhouse and Jason Blazakis and Kris McGuffie and Jasmine Wang},
  year          = {2019},
  eprint        = {1908.09203},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{srivastava2014dropout,
  title     = {Dropout: a simple way to prevent neural networks from overfitting},
  author    = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal   = {The journal of machine learning research},
  volume    = {15},
  number    = {1},
  pages     = {1929--1958},
  year      = {2014},
  publisher = {JMLR. org}
}
@article{su2024roformer,
  title     = {Roformer: Enhanced transformer with rotary position embedding},
  author    = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal   = {Neurocomputing},
  volume    = {568},
  pages     = {127063},
  year      = {2024},
  publisher = {Elsevier}
}
@article{sun2023principle,
  title   = {Principle-driven self-alignment of language models from scratch with minimal human supervision},
  author  = {Sun, Zhiqing and Shen, Yikang and Zhou, Qinhong and Zhang, Hongxin and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
  journal = {arXiv preprint arXiv:2305.03047},
  year    = {2023}
}
@article{tay2022unifying,
  title   = {Unifying language learning paradigms},
  author  = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},
  journal = {arXiv preprint arXiv:2205.05131},
  year    = {2022}
}
@article{thoppilan2022lamda,
  title   = {Lamda: Language models for dialog applications},
  author  = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal = {arXiv preprint arXiv:2201.08239},
  year    = {2022}
}
@article{touvron2023llama,
  title   = {Llama 2: Open foundation and fine-tuned chat models},
  author  = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal = {arXiv preprint arXiv:2307.09288},
  year    = {2023}
}
@misc{uesato2022solving,
  title         = {Solving math word problems with process- and outcome-based feedback},
  author        = {Jonathan Uesato and Nate Kushman and Ramana Kumar and Francis Song and Noah Siegel and Lisa Wang and Antonia Creswell and Geoffrey Irving and Irina Higgins},
  year          = {2022},
  eprint        = {2211.14275},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{varshney2023stitch,
  title         = {A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation},
  author        = {Neeraj Varshney and Wenlin Yao and Hongming Zhang and Jianshu Chen and Dong Yu},
  year          = {2023},
  eprint        = {2307.03987},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}
@article{wang2022deepnet,
  title   = {Deepnet: Scaling transformers to 1,000 layers},
  author  = {Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu},
  journal = {arXiv preprint arXiv:2203.00555},
  year    = {2022}
}
@inproceedings{wang2022language,
  title        = {What language model architecture and pretraining objective works best for zero-shot generalization?},
  author       = {Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Le Scao, Teven and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin},
  booktitle    = {International Conference on Machine Learning},
  pages        = {22964--22984},
  year         = {2022},
  organization = {PMLR}
}
@article{wang2022self,
  title   = {Self-consistency improves chain of thought reasoning in language models},
  author  = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal = {arXiv preprint arXiv:2203.11171},
  year    = {2022}
}
@article{wang2022super,
  title   = {Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks},
  author  = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal = {arXiv preprint arXiv:2204.07705},
  year    = {2022}
}
@misc{wang2023aligning,
  title         = {Aligning Large Language Models with Human: A Survey},
  author        = {Yufei Wang and Wanjun Zhong and Liangyou Li and Fei Mi and Xingshan Zeng and Wenyong Huang and Lifeng Shang and Xin Jiang and Qun Liu},
  year          = {2023},
  eprint        = {2307.12966},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{wang2023selfconsistency,
  title         = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author        = {Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
  year          = {2023},
  eprint        = {2203.11171},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{webster1992tokenization,
  title     = {Tokenization as the initial phase in NLP},
  author    = {Webster, Jonathan J and Kit, Chunyu},
  booktitle = {COLING 1992 volume 4: The 14th international conference on computational linguistics},
  year      = {1992}
}
@article{wei2022chain,
  title   = {Chain-of-thought prompting elicits reasoning in large language models},
  author  = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {24824--24837},
  year    = {2022}
}
@misc{wei2023chainofthought,
  title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author        = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
  year          = {2023},
  eprint        = {2201.11903},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{welleck2022generating,
  title         = {Generating Sequences by Learning to Self-Correct},
  author        = {Sean Welleck and Ximing Lu and Peter West and Faeze Brahman and Tianxiao Shen and Daniel Khashabi and Yejin Choi},
  year          = {2022},
  eprint        = {2211.00053},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{weng2023large,
  title         = {Large Language Models are Better Reasoners with Self-Verification},
  author        = {Yixuan Weng and Minjun Zhu and Fei Xia and Bin Li and Shizhu He and Shengping Liu and Bin Sun and Kang Liu and Jun Zhao},
  year          = {2023},
  eprint        = {2212.09561},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@misc{wu2021textgail,
  title         = {TextGAIL: Generative Adversarial Imitation Learning for Text Generation},
  author        = {Qingyang Wu and Lei Li and Zhou Yu},
  year          = {2021},
  eprint        = {2004.13796},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{xie2023selfevaluation,
  title         = {Self-Evaluation Guided Beam Search for Reasoning},
  author        = {Yuxi Xie and Kenji Kawaguchi and Yiran Zhao and Xu Zhao and Min-Yen Kan and Junxian He and Qizhe Xie},
  year          = {2023},
  eprint        = {2305.00633},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{xue2020mt5,
  title   = {mT5: A massively multilingual pre-trained text-to-text transformer},
  author  = {Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  journal = {arXiv preprint arXiv:2010.11934},
  year    = {2020}
}
@article{yan2023learning,
  title   = {Learning to simulate natural language feedback for interactive semantic parsing},
  author  = {Yan, Hao and Srivastava, Saurabh and Tai, Yintao and Wang, Sida I and Yih, Wen-tau and Yao, Ziyu},
  journal = {arXiv preprint arXiv:2305.08195},
  year    = {2023}
}
@article{yang2021fudge,
  title   = {FUDGE: Controlled text generation with future discriminators},
  author  = {Yang, Kevin and Klein, Dan},
  journal = {arXiv preprint arXiv:2104.05218},
  year    = {2021}
}
@misc{yao2023tree,
  title         = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  author        = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
  year          = {2023},
  eprint        = {2305.10601},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{yu2023improving,
  title         = {Improving Language Models via Plug-and-Play Retrieval Feedback},
  author        = {Wenhao Yu and Zhihan Zhang and Zhenwen Liang and Meng Jiang and Ashish Sabharwal},
  year          = {2023},
  eprint        = {2305.14002},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{zelikman2022star,
  title         = {STaR: Bootstrapping Reasoning With Reasoning},
  author        = {Eric Zelikman and Yuhuai Wu and Jesse Mu and Noah D. Goodman},
  year          = {2022},
  eprint        = {2203.14465},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{zhang2019root,
  title   = {Root mean square layer normalization},
  author  = {Zhang, Biao and Sennrich, Rico},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {32},
  year    = {2019}
}
@misc{zhang2020bertscore,
  title         = {BERTScore: Evaluating Text Generation with BERT},
  author        = {Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
  year          = {2020},
  eprint        = {1904.09675},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{zhang2023algo,
  title         = {ALGO: Synthesizing Algorithmic Programs with LLM-Generated Oracle Verifiers},
  author        = {Kexun Zhang and Danqing Wang and Jingtao Xia and William Yang Wang and Lei Li},
  year          = {2023},
  eprint        = {2305.14591},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{zhang2023language,
  title         = {How Language Model Hallucinations Can Snowball},
  author        = {Muru Zhang and Ofir Press and William Merrill and Alisa Liu and Noah A. Smith},
  year          = {2023},
  eprint        = {2305.13534},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{zhang2023selfedit,
  title         = {Self-Edit: Fault-Aware Code Editor for Code Generation},
  author        = {Kechi Zhang and Zhuo Li and Jia Li and Ge Li and Zhi Jin},
  year          = {2023},
  eprint        = {2305.04087},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE}
}
@article{zhao2023survey,
  title   = {A survey of large language models},
  author  = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal = {arXiv preprint arXiv:2303.18223},
  year    = {2023}
}
@article{zhu2022solving,
  title   = {Solving math word problems via cooperative reasoning induced language models},
  author  = {Zhu, Xinyu and Wang, Junjie and Zhang, Lin and Zhang, Yuxiang and Huang, Yongfeng and Gan, Ruyi and Zhang, Jiaxing and Yang, Yujiu},
  journal = {arXiv preprint arXiv:2210.16257},
  year    = {2022}
}
@article{ziegler2019fine,
  title   = {Fine-tuning language models from human preferences},
  author  = {Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal = {arXiv preprint arXiv:1909.08593},
  year    = {2019}
}
@misc{jiang2023selfevolve,
  title         = {SelfEvolve: A Code Evolution Framework via Large Language Models},
  author        = {Shuyang Jiang and Yuhao Wang and Yu Wang},
  year          = {2023},
  eprint        = {2306.02907},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{robertson1994some,
  title        = {Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval},
  author       = {Robertson, Stephen E and Walker, Steve},
  booktitle    = {SIGIR’94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, organised by Dublin City University},
  pages        = {232--241},
  year         = {1994},
  organization = {Springer}
}
@article{reimers2019sentence,
  title   = {Sentence-bert: Sentence embeddings using siamese bert-networks},
  author  = {Reimers, Nils and Gurevych, Iryna},
  journal = {arXiv preprint arXiv:1908.10084},
  year    = {2019}
}
@article{gao2021simcse,
  title   = {Simcse: Simple contrastive learning of sentence embeddings},
  author  = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  journal = {arXiv preprint arXiv:2104.08821},
  year    = {2021}
}
@inproceedings{borgeaud2022improving,
  title        = {Improving language models by retrieving from trillions of tokens},
  author       = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle    = {International conference on machine learning},
  pages        = {2206--2240},
  year         = {2022},
  organization = {PMLR}
}
@article{li2023context,
  title   = {In-context learning with many demonstration examples},
  author  = {Li, Mukai and Gong, Shansan and Feng, Jiangtao and Xu, Yiheng and Zhang, Jun and Wu, Zhiyong and Kong, Lingpeng},
  journal = {arXiv preprint arXiv:2302.04931},
  year    = {2023}
}
@article{geva2020transformer,
  title   = {Transformer feed-forward layers are key-value memories},
  author  = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  journal = {arXiv preprint arXiv:2012.14913},
  year    = {2020}
}
@article{zhang2022automatic,
  title   = {Automatic chain of thought prompting in large language models},
  author  = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  journal = {arXiv preprint arXiv:2210.03493},
  year    = {2022}
}
