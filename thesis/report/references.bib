@article{abid2021persistent,
  author = {Abubakar Abid and Maheen Farooqi and James Zou},
  note   = {Internet: https://arxiv.org/abs/2101.05783, Apr. 15, 2024},
  title  = {Persistent Anti-Muslim Bias in Large Language Models}
}

@article{akyurek-etal-2023-rl4f,
  author = {Akyürek, Afra Feyza and Akyürek, Ekin and Madaan, Aman and Kalyan, Ashwin and Clark, Peter and Wijaya, Derry and Tandon, Niket},
  note   = {Internet: https://arxiv.org/abs/2305.08844, Apr. 20, 2024},
  title  = {Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs}
}

@article{askell2021general,
  author = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  note   = {Internet: https://arxiv.org/abs/2112.00861, Apr. 17, 2024},
  title  = {A general language assistant as a laboratory for alignment}
}

@article{baevski2018adaptive,
  author = {Baevski, Alexei and Auli, Michael},
  note   = {Internet: https://arxiv.org/abs/1809.10853, Apr. 13, 2024},
  title  = {Adaptive input representations for neural language modeling}
}

@article{bahdanau2014neural,
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  note   = {Internet: https://arxiv.org/abs/1409.0473, Apr. 22, 2024},
  title  = {Neural machine translation by jointly learning to align and translate}
}

@article{bai2022constitutional,
  author = {Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
  note   = {Internet: https://arxiv.org/abs/2212.08073, Apr. 13, 2024},
  title  = {Constitutional AI: Harmlessness from AI Feedback}
}

@article{bai2022training,
  author = {Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
  note   = {Internet: https://arxiv.org/abs/2204.05862, Apr. 30, 2024},
  title  = {Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback}
}

@inproceedings{borgeaud2022improving,
  author       = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle    = {International conference on machine learning},
  organization = {PMLR},
  pages        = {2206--2240},
  title        = {Improving language models by retrieving from trillions of tokens},
  year         = {2022}
}

@article{brown2020language,
  author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  note   = {Internet: https://arxiv.org/abs/2005.14165, Apr. 28, 2024},
  title  = {Language Models are Few-Shot Learners}
}

@article{chang2023learning,
  author = {Jonathan D. Chang and Kiante Brantley and Rajkumar Ramamurthy and Dipendra Misra and Wen Sun},
  note   = {Internet: https://arxiv.org/abs/2306.11816, Apr. 09, 2024},
  title  = {Learning to Generate Better Than Your LLM}
}

@article{charalambous2023new,
  author = {Yiannis Charalambous and Norbert Tihanyi and Ridhi Jain and Youcheng Sun and Mohamed Amine Ferrag and Lucas C. Cordeiro},
  note   = {Internet: https://arxiv.org/abs/2305.14752, Apr. 30, 2024},
  title  = {A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification}
}

@article{chen2021evaluating,
  author = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  note   = {Internet: https://arxiv.org/abs/2107.03374, Apr. 17, 2024},
  title  = {Evaluating Large Language Models Trained on Code}
}

@article{chen2022codet,
  author = {Bei Chen and Fengji Zhang and Anh Nguyen and Daoguang Zan and Zeqi Lin and Jian-Guang Lou and Weizhu Chen},
  note   = {Internet: https://arxiv.org/abs/2207.10397, Apr. 06, 2024},
  title  = {CodeT: Code Generation with Generated Tests}
}

@article{chen2023improving,
  author = {Angelica Chen and Jérémy Scheurer and Tomasz Korbak and Jon Ander Campos and Jun Shern Chan and Samuel R. Bowman and Kyunghyun Cho and Ethan Perez},
  note   = {Internet: https://arxiv.org/abs/2303.16749, Apr. 04, 2024},
  title  = {Improving Code Generation by Training with Natural Language Feedback}
}

@article{chen2023teaching,
  author = {Xinyun Chen and Maxwell Lin and Nathanael Schärli and Denny Zhou},
  note   = {Internet: https://arxiv.org/abs/2304.05128, Apr. 19, 2024},
  title  = {Teaching Large Language Models to Self-Debug}
}

@article{chern2023factool,
  author = {I-Chun Chern and Steffi Chern and Shiqi Chen and Weizhe Yuan and Kehua Feng and Chunting Zhou and Junxian He and Graham Neubig and Pengfei Liu},
  note   = {Internet: https://arxiv.org/abs/2307.13528, Apr. 04, 2024},
  title  = {FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios}
}

@article{child2019generating,
  author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  note   = {Internet: https://arxiv.org/abs/1904.10509, Apr. 08, 2024},
  title  = {Generating long sequences with sparse transformers}
}

@article{chowdhery2023palm,
  author  = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal = {Journal of Machine Learning Research},
  number  = {240},
  pages   = {1--113},
  title   = {Palm: Scaling language modeling with pathways},
  volume  = {24},
  year    = {2023}
}

@article{chung2022scaling,
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  note   = {Internet: https://arxiv.org/abs/2210.11416, Apr. 26, 2024},
  title  = {Scaling instruction-finetuned language models}
}

@article{clark2021thats,
  author = {Elizabeth Clark and Tal August and Sofia Serrano and Nikita Haduong and Suchin Gururangan and Noah A. Smith},
  note   = {Internet: https://arxiv.org/abs/2107.00061, Apr. 06, 2024},
  title  = {All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text}
}

@article{cobbe2021training,
  author = {Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
  note   = {Internet: https://arxiv.org/abs/2110.14168, Apr. 01, 2024},
  title  = {Training Verifiers to Solve Math Word Problems}
}

@article{cohen2023lm,
  author = {Roi Cohen and May Hamri and Mor Geva and Amir Globerson},
  note   = {Internet: https://arxiv.org/abs/2305.13281, Apr. 15, 2024},
  title  = {LM vs LM: Detecting Factual Errors via Cross Examination}
}

@article{creswell2022faithful,
  author = {Antonia Creswell and Murray Shanahan},
  note   = {Internet: https://arxiv.org/abs/2208.14271, Apr. 14, 2024},
  title  = {Faithful Reasoning Using Large Language Models}
}

@article{dao2022flashattention,
  author  = {Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
  journal = {Advances in Neural Information Processing Systems},
  pages   = {16344--16359},
  title   = {Flashattention: Fast and memory-efficient exact attention with io-awareness},
  volume  = {35},
  year    = {2022}
}

@article{dathathri2020plug,
  author = {Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},
  note   = {Internet: https://arxiv.org/abs/1912.02164, Apr. 22, 2024},
  title  = {Plug and Play Language Models: A Simple Approach to Controlled Text Generation}
}

@inproceedings{dauphin2017language,
  author       = {Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle    = {International conference on machine learning},
  organization = {PMLR},
  pages        = {933--941},
  title        = {Language modeling with gated convolutional networks},
  year         = {2017}
}

@article{devlin2019bert,
  author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  note   = {Internet: https://arxiv.org/abs/1810.04805, Apr. 08, 2024},
  title  = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}
}

@article{dong2022survey,
  author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  note   = {Internet: https://arxiv.org/abs/2301.00234, Apr. 14, 2024},
  title  = {A survey for in-context learning}
}

@article{du2023improving,
  author = {Yilun Du and Shuang Li and Antonio Torralba and Joshua B. Tenenbaum and Igor Mordatch},
  note   = {Internet: https://arxiv.org/abs/2305.14325, Apr. 25, 2024},
  title  = {Improving Factuality and Reasoning in Language Models through Multiagent Debate}
}

@article{dubois2024alpacafarm,
  author = {Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
  note   = {Internet: https://arxiv.org/abs/2305.14387, Apr. 17, 2024},
  title  = {AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback}
}

@article{eriksen1972some,
  author    = {Eriksen, Charles W and Hoffman, James E},
  journal   = {Perception & Psychophysics},
  number    = {2},
  pages     = {169--171},
  publisher = {Springer},
  title     = {Some characteristics of selective attention in visual perception determined by vocal reaction time},
  volume    = {11},
  year      = {1972}
}

@article{fernandes2023bridging,
  author = {Patrick Fernandes and Aman Madaan and Emmy Liu and António Farinhas and Pedro Henrique Martins and Amanda Bertsch and José G. C. de Souza and Shuyan Zhou and Tongshuang Wu and Graham Neubig and André F. T. Martins},
  note   = {Internet: https://arxiv.org/abs/2305.00955, Apr. 28, 2024},
  title  = {Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation}
}

@article{first2023baldur,
  author = {Emily First and Markus N. Rabe and Talia Ringer and Yuriy Brun},
  note   = {Internet: https://arxiv.org/abs/2303.04910, Apr. 06, 2024},
  title  = {Baldur: Whole-Proof Generation and Repair with Large Language Models}
}

@article{freitag-etal-2022-high,
  author    = {Freitag, Markus and Grangier, David and Tan, Qijun and Liang, Bowen},
  journal   = {Transactions of the Association for Computational Linguistics},
  pages     = {811--825},
  publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…},
  title     = {High quality rather than high model probability: Minimum Bayes risk decoding with neural metrics},
  volume    = {10},
  year      = {2022}
}

@article{fu2023improving,
  author = {Yao Fu and Hao Peng and Tushar Khot and Mirella Lapata},
  note   = {Internet: https://arxiv.org/abs/2305.10142, Apr. 03, 2024},
  title  = {Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback}
}

@article{ganguli2023capacity,
  author = {Deep Ganguli and Amanda Askell and Nicholas Schiefer and Thomas I. Liao and Kamilė Lukošiūtė and Anna Chen and Anna Goldie and Azalia Mirhoseini and Catherine Olsson and Danny Hernandez and Dawn Drain and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jackson Kernion and Jamie Kerr and Jared Mueller and Joshua Landau and Kamal Ndousse and Karina Nguyen and Liane Lovitt and Michael Sellitto and Nelson Elhage and Noemi Mercado and Nova DasSarma and Oliver Rausch and Robert Lasenby and Robin Larson and Sam Ringer and Sandipan Kundu and Saurav Kadavath and Scott Johnston and Shauna Kravec and Sheer El Showk and Tamera Lanham and Timothy Telleen-Lawton and Tom Henighan and Tristan Hume and Yuntao Bai and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Christopher Olah and Jack Clark and Samuel R. Bowman and Jared Kaplan},
  note   = {Internet: https://arxiv.org/abs/2302.07459, Apr. 26, 2024},
  title  = {The Capacity for Moral Self-Correction in Large Language Models}
}

@article{gao2021simcse,
  author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  note   = {Internet: https://arxiv.org/abs/2104.08821, Apr. 15, 2024},
  title  = {Simcse: Simple contrastive learning of sentence embeddings}
}

@article{gao2023continually,
  author = {Ge Gao and Hung-Ting Chen and Yoav Artzi and Eunsol Choi},
  note   = {Internet: https://arxiv.org/abs/2305.12473, Apr. 24, 2024},
  title  = {Continually Improving Extractive QA via Human Feedback}
}

@article{gao2023rarr,
  author = {Luyu Gao and Zhuyun Dai and Panupong Pasupat and Anthony Chen and Arun Tejasvi Chaganty and Yicheng Fan and Vincent Y. Zhao and Ni Lao and Hongrae Lee and Da-Cheng Juan and Kelvin Guu},
  note   = {Internet: https://arxiv.org/abs/2210.08726, Apr. 20, 2024},
  title  = {RARR: Researching and Revising What Language Models Say, Using Language Models}
}

@article{gero2023selfverification,
  author = {Zelalem Gero and Chandan Singh and Hao Cheng and Tristan Naumann and Michel Galley and Jianfeng Gao and Hoifung Poon},
  note   = {Internet: https://arxiv.org/abs/2306.00024, Apr. 24, 2024},
  title  = {Self-Verification Improves Few-Shot Clinical Information Extraction}
}

@article{geva2020transformer,
  author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  note   = {Internet: https://arxiv.org/abs/2012.14913, Apr. 16, 2024},
  title  = {Transformer feed-forward layers are key-value memories}
}

@article{glaese2022improving,
  author = {Amelia Glaese and Nat McAleese and Maja Trebacz and John Aslanides and Vlad Firoiu and Timo Ewalds and Maribeth Rauh and Laura Weidinger and Martin Chadwick and Phoebe Thacker and Lucy Campbell-Gillingham and Jonathan Uesato and Po-Sen Huang and Ramona Comanescu and Fan Yang and Abigail See and Sumanth Dathathri and Rory Greig and Charlie Chen and Doug Fritz and Jaume Sanchez Elias and Richard Green and Sona Mokra and Nicholas Fernando and Boxi Wu and Rachel Foley and Susannah Young and Iason Gabriel and William Isaac and John Mellor and Demis Hassabis and Koray Kavukcuoglu and Lisa Anne Hendricks and Geoffrey Irving},
  note   = {Internet: https://arxiv.org/abs/2209.14375, Apr. 16, 2024},
  title  = {Improving alignment of dialogue agents via targeted human judgements}
}

@article{goldstein2023generative,
  author = {Josh A. Goldstein and Girish Sastry and Micah Musser and Renee DiResta and Matthew Gentzel and Katerina Sedova},
  note   = {Internet: https://arxiv.org/abs/2301.04246, Apr. 12, 2024},
  title  = {Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations}
}

@article{golovneva2023roscoe,
  author = {Olga Golovneva and Moya Chen and Spencer Poff and Martin Corredor and Luke Zettlemoyer and Maryam Fazel-Zarandi and Asli Celikyilmaz},
  note   = {Internet: https://arxiv.org/abs/2212.07919, Apr. 10, 2024},
  title  = {ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning}
}

@article{gou2023critic,
  author = {Zhibin Gou and Zhihong Shao and Yeyun Gong and Yelong Shen and Yujiu Yang and Nan Duan and Weizhu Chen},
  note   = {Internet: https://arxiv.org/abs/2305.11738, Apr. 27, 2024},
  title  = {CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing}
}

@article{gulcehre2023reinforced,
  author = {Caglar Gulcehre and Tom Le Paine and Srivatsan Srinivasan and Ksenia Konyushkova and Lotte Weerts and Abhishek Sharma and Aditya Siddhant and Alex Ahern and Miaosen Wang and Chenjie Gu and Wolfgang Macherey and Arnaud Doucet and Orhan Firat and Nando de Freitas},
  note   = {Internet: https://arxiv.org/abs/2308.08998, Apr. 23, 2024},
  title  = {Reinforced Self-Training (ReST) for Language Modeling}
}

@article{hao2023reasoning,
  author = {Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and Daisy Zhe Wang and Zhiting Hu},
  note   = {Internet: https://arxiv.org/abs/2305.14992, Apr. 10, 2024},
  title  = {Reasoning with Language Model is Planning with World Model}
}

@article{he2021deberta,
  author = {Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
  note   = {Internet: https://arxiv.org/abs/2006.03654, Apr. 08, 2024},
  title  = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention}
}

@article{he2022rethinking,
  author = {Hangfeng He and Hongming Zhang and Dan Roth},
  note   = {Internet: https://arxiv.org/abs/2301.00303, Apr. 11, 2024},
  title  = {Rethinking with Retrieval: Faithful Large Language Model Inference}
}

@article{hendrycks2016gaussian,
  author = {Hendrycks, Dan and Gimpel, Kevin},
  note   = {Internet: https://arxiv.org/abs/1606.08415, Apr. 07, 2024},
  title  = {Gaussian error linear units (gelus)}
}

@article{hendrycks2021measuring,
  author = {Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Song and Jacob Steinhardt},
  note   = {Internet: https://arxiv.org/abs/2105.09938, Apr. 14, 2024},
  title  = {Measuring Coding Challenge Competence With APPS}
}

@article{hernandez2021scaling,
  author = {Danny Hernandez and Jared Kaplan and Tom Henighan and Sam McCandlish},
  note   = {Internet: https://arxiv.org/abs/2102.01293, Apr. 01, 2024},
  title  = {Scaling Laws for Transfer}
}

@article{hornik1989multilayer,
  author    = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal   = {Neural networks},
  number    = {5},
  pages     = {359--366},
  publisher = {Elsevier},
  title     = {Multilayer feedforward networks are universal approximators},
  volume    = {2},
  year      = {1989}
}

@article{huang2022large,
  author = {Jiaxin Huang and Shixiang Shane Gu and Le Hou and Yuexin Wu and Xuezhi Wang and Hongkun Yu and Jiawei Han},
  note   = {Internet: https://arxiv.org/abs/2210.11610, Apr. 04, 2024},
  title  = {Large Language Models Can Self-Improve}
}

@article{huang2022towards,
  author = {Huang, Jie and Chang, Kevin Chen-Chuan},
  note   = {Internet: https://arxiv.org/abs/2212.10403, Apr. 01, 2024},
  title  = {Towards reasoning in large language models: A survey}
}

@article{iu-liu-2021-simcls,
  author = {Liu, Yixin and Liu, Pengfei},
  note   = {Internet: https://arxiv.org/abs/2106.01890, Apr. 26, 2024},
  title  = {SimCLS: A simple framework for contrastive learning of abstractive summarization}
}

@article{iyer2022opt,
  author = {Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and others},
  note   = {Internet: https://arxiv.org/abs/2212.12017, Apr. 24, 2024},
  title  = {Opt-iml: Scaling language model instruction meta learning through the lens of generalization}
}

@article{jang2017categorical,
  author = {Eric Jang and Shixiang Gu and Ben Poole},
  note   = {Internet: https://arxiv.org/abs/1611.01144, Apr. 04, 2024},
  title  = {Categorical Reparameterization with Gumbel-Softmax}
}

@article{jauregi-unanue-etal-2021-berttune,
  author = {Unanue, Inigo Jauregi and Parnell, Jacob and Piccardi, Massimo},
  note   = {Internet: https://arxiv.org/abs/2106.02208, Apr. 29, 2024},
  title  = {BERTTune: Fine-tuning neural machine translation with BERTScore}
}

@article{jiang2023selfevolve,
  author = {Shuyang Jiang and Yuhao Wang and Yu Wang},
  note   = {Internet: https://arxiv.org/abs/2306.02907, Apr. 07, 2024},
  title  = {SelfEvolve: A Code Evolution Framework via Large Language Models}
}

@article{jung-etal-2022-maieutic,
  author = {Jung, Jaehun and Qin, Lianhui and Welleck, Sean and Brahman, Faeze and Bhagavatula, Chandra and Bras, Ronan Le and Choi, Yejin},
  note   = {Internet: https://arxiv.org/abs/2205.11822, Apr. 20, 2024},
  title  = {Maieutic prompting: Logically consistent reasoning with recursive explanations}
}

@article{kaplan2020scaling,
  author = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  note   = {Internet: https://arxiv.org/abs/2001.08361, Apr. 23, 2024},
  title  = {Scaling Laws for Neural Language Models}
}

@article{kenton2021alignment,
  author = {Zachary Kenton and Tom Everitt and Laura Weidinger and Iason Gabriel and Vladimir Mikulik and Geoffrey Irving},
  note   = {Internet: https://arxiv.org/abs/2103.14659, Apr. 30, 2024},
  title  = {Alignment of Language Agents}
}

@article{keskar2019ctrl,
  author = {Nitish Shirish Keskar and Bryan McCann and Lav R. Varshney and Caiming Xiong and Richard Socher},
  note   = {Internet: https://arxiv.org/abs/1909.05858, Apr. 16, 2024},
  title  = {CTRL: A Conditional Transformer Language Model for Controllable Generation}
}

@article{khalifa2023grace,
  author = {Muhammad Khalifa and Lajanugen Logeswaran and Moontae Lee and Honglak Lee and Lu Wang},
  note   = {Internet: https://arxiv.org/abs/2305.14934, Apr. 11, 2024},
  title  = {GRACE: Discriminator-Guided Chain-of-Thought Reasoning}
}

@article{khlaaf2022hazard,
  author = {Heidy Khlaaf and Pamela Mishkin and Joshua Achiam and Gretchen Krueger and Miles Brundage},
  note   = {Internet: https://arxiv.org/abs/2207.14157, Apr. 03, 2024},
  title  = {A Hazard Analysis Framework for Code Synthesis Large Language Models}
}

@article{kim2023cot,
  author = {Kim, Seungone and Joo, Se June and Kim, Doyoung and Jang, Joel and Ye, Seonghyeon and Shin, Jamin and Seo, Minjoon},
  note   = {Internet: https://arxiv.org/abs/2305.14045, Apr. 27, 2024},
  title  = {The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning}
}

@article{korbak2023pretraining,
  author = {Tomasz Korbak and Kejian Shi and Angelica Chen and Rasika Bhalerao and Christopher L. Buckley and Jason Phang and Samuel R. Bowman and Ethan Perez},
  note   = {Internet: https://arxiv.org/abs/2302.08582, Apr. 10, 2024},
  title  = {Pretraining Language Models with Human Preferences}
}

@article{krueger2016zoneout,
  author = {Krueger, David and Maharaj, Tegan and Kramár, János and Pezeshki, Mohammad and Ballas, Nicolas and Ke, Nan Rosemary and Goyal, Anirudh and Bengio, Yoshua and Courville, Aaron and Pal, Chris},
  note   = {Internet: https://arxiv.org/abs/1606.01305, Apr. 18, 2024},
  title  = {Zoneout: Regularizing rnns by randomly preserving hidden activations}
}

@article{le2022coderl,
  author = {Hung Le and Yue Wang and Akhilesh Deepak Gotmare and Silvio Savarese and Steven C. H. Hoi},
  note   = {Internet: https://arxiv.org/abs/2207.01780, Apr. 12, 2024},
  title  = {CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning}
}

@article{le2023codechain,
  author = {Hung Le and Hailin Chen and Amrita Saha and Akash Gokul and Doyen Sahoo and Shafiq Joty},
  note   = {Internet: https://arxiv.org/abs/2310.08992, Apr. 19, 2024},
  title  = {CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules}
}

@article{li-etal-2019-deep,
  author = {Li, Siyao and Lei, Deren and Qin, Pengda and Wang, William Yang},
  note   = {Internet: https://arxiv.org/abs/1909.00141, Apr. 30, 2024},
  title  = {Deep reinforcement learning with distributional semantic rewards for abstractive summarization}
}

@article{li-etal-2023-making,
  author = {Li, Yifei  and
            Lin, Zeqi  and
            Zhang, Shizhuo  and
            Fu, Qiang  and
            Chen, Bei  and
            Lou, Jian-Guang  and
            Chen, Weizhu},
  note   = {Internet: https://arxiv.org/abs/2206.02336, Apr. 06, 2024},
  title  = {Making Language Models Better Reasoners with Step-Aware Verifier}
}

@article{li2022diffusionlm,
  author = {Xiang Lisa Li and John Thickstun and Ishaan Gulrajani and Percy Liang and Tatsunori B. Hashimoto},
  note   = {Internet: https://arxiv.org/abs/2205.14217, Apr. 06, 2024},
  title  = {Diffusion-LM Improves Controllable Text Generation}
}

@article{li2023context,
  author = {Li, Mukai and Gong, Shansan and Feng, Jiangtao and Xu, Yiheng and Zhang, Jun and Wu, Zhiyong and Kong, Lingpeng},
  note   = {Internet: https://arxiv.org/abs/2302.04931, Apr. 29, 2024},
  title  = {In-context learning with many demonstration examples}
}

@article{li2023halueval,
  author = {Junyi Li and Xiaoxue Cheng and Wayne Xin Zhao and Jian-Yun Nie and Ji-Rong Wen},
  note   = {Internet: https://arxiv.org/abs/2305.11747, Apr. 09, 2024},
  title  = {HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models}
}

@article{li2023prd,
  author = {Ruosen Li and Teerth Patel and Xinya Du},
  note   = {Internet: https://arxiv.org/abs/2307.02762, Apr. 29, 2024},
  title  = {PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations}
}

@article{Li_2022,
  author    = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, Rémi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and Hubert, Thomas and Choy, Peter and de Masson d’Autume, Cyprien and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Sutherland Robson, Esme and Kohli, Pushmeet and de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
  doi       = {10.1126/science.abq1158},
  issn      = {1095-9203},
  journal   = {Science},
  month     = {December},
  number    = {6624},
  pages     = {1092–1097},
  publisher = {American Association for the Advancement of Science (AAAS)},
  title     = {Competition-level code generation with AlphaCode},
  url       = {http://dx.doi.org/10.1126/science.abq1158},
  volume    = {378},
  year      = {2022}
}

@article{lightman2023lets,
  author = {Hunter Lightman and Vineet Kosaraju and Yura Burda and Harri Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
  note   = {Internet: https://arxiv.org/abs/2305.20050, Apr. 23, 2024},
  title  = {Let's Verify Step by Step}
}

@article{liu2018generating,
  author = {Liu, Peter J and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
  note   = {Internet: https://arxiv.org/abs/1801.10198, Apr. 16, 2024},
  title  = {Generating wikipedia by summarizing long sequences}
}

@article{liu2023chain,
  author = {Hao Liu and Carmelo Sferrazza and Pieter Abbeel},
  note   = {Internet: https://arxiv.org/abs/2302.02676, Apr. 13, 2024},
  title  = {Chain of Hindsight Aligns Language Models with Feedback}
}

@article{liu2023zero,
  author = {Liu, Qian and Zhou, Fan and Jiang, Zhengbao and Dou, Longxu and Lin, Min},
  note   = {Internet: https://arxiv.org/abs/2304.07995, Apr. 20, 2024},
  title  = {From zero to hero: Examining the power of symbolic tasks in instruction tuning}
}

@article{lu2022quark,
  author = {Ximing Lu and Sean Welleck and Jack Hessel and Liwei Jiang and Lianhui Qin and Peter West and Prithviraj Ammanabrolu and Yejin Choi},
  note   = {Internet: https://arxiv.org/abs/2205.13636, Apr. 29, 2024},
  title  = {Quark: Controllable Text Generation with Reinforced Unlearning}
}

@article{luo2023wizardcoder,
  author = {Ziyang Luo and Can Xu and Pu Zhao and Qingfeng Sun and Xiubo Geng and Wenxiang Hu and Chongyang Tao and Jing Ma and Qingwei Lin and Daxin Jiang},
  note   = {Internet: https://arxiv.org/abs/2306.08568, Apr. 16, 2024},
  title  = {WizardCoder: Empowering Code Large Language Models with Evol-Instruct}
}

@article{lyu2023faithful,
  author = {Qing Lyu and Shreya Havaldar and Adam Stein and Li Zhang and Delip Rao and Eric Wong and Marianna Apidianaki and Chris Callison-Burch},
  note   = {Internet: https://arxiv.org/abs/2301.13379, Apr. 25, 2024},
  title  = {Faithful Chain-of-Thought Reasoning}
}

@article{madaan-etal-2022-memory,
  author = {Madaan, Aman and Tandon, Niket and Clark, Peter and Yang, Yiming},
  note   = {Internet: https://arxiv.org/abs/2201.06009, Apr. 24, 2024},
  title  = {Memory-assisted prompt editing to improve GPT-3 after deployment}
}

@article{madaan2022language,
  author = {Aman Madaan and Shuyan Zhou and Uri Alon and Yiming Yang and Graham Neubig},
  note   = {Internet: https://arxiv.org/abs/2210.07128, Apr. 19, 2024},
  title  = {Language Models of Code are Few-Shot Commonsense Learners}
}

@article{madaan2023selfrefine,
  author = {Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark},
  note   = {Internet: https://arxiv.org/abs/2303.17651, Apr. 16, 2024},
  title  = {Self-Refine: Iterative Refinement with Self-Feedback}
}

@article{mehrabi2023flirt,
  author = {Ninareh Mehrabi and Palash Goyal and Christophe Dupuy and Qian Hu and Shalini Ghosh and Richard Zemel and Kai-Wei Chang and Aram Galstyan and Rahul Gupta},
  note   = {Internet: https://arxiv.org/abs/2308.04265, Apr. 28, 2024},
  title  = {FLIRT: Feedback Loop In-context Red Teaming}
}

@article{mialon2023augmented,
  author = {Grégoire Mialon and Roberto Dessì and Maria Lomeli and Christoforos Nalmpantis and Ram Pasunuru and Roberta Raileanu and Baptiste Rozière and Timo Schick and Jane Dwivedi-Yu and Asli Celikyilmaz and Edouard Grave and Yann LeCun and Thomas Scialom},
  note   = {Internet: https://arxiv.org/abs/2302.07842, Apr. 19, 2024},
  title  = {Augmented Language Models: a Survey}
}

@article{mielke2021between,
  author = {Mielke, Sabrina J and Alyafeai, Zaid and Salesky, Elizabeth and Raffel, Colin and Dey, Manan and Gallé, Matthias and Raja, Arun and Si, Chenglei and Lee, Wilson Y and Sagot, Benoı̂t and others},
  note   = {Internet: https://arxiv.org/abs/2112.10508, Apr. 02, 2024},
  title  = {Between words and characters: a brief history of open-vocabulary modeling and tokenization in nlp}
}

@article{ni2023lever,
  author = {Ansong Ni and Srini Iyer and Dragomir Radev and Ves Stoyanov and Wen-tau Yih and Sida I. Wang and Xi Victoria Lin},
  note   = {Internet: https://arxiv.org/abs/2302.08468, Apr. 11, 2024},
  title  = {LEVER: Learning to Verify Language-to-Code Generation with Execution}
}

@article{olausson2023selfrepair,
  author = {Theo X. Olausson and Jeevana Priya Inala and Chenglong Wang and Jianfeng Gao and Armando Solar-Lezama},
  note   = {Internet: https://arxiv.org/abs/2306.09896, Apr. 05, 2024},
  title  = {Is Self-Repair a Silver Bullet for Code Generation?}
}

@article{openai2023gpt4,
  author = {OpenAI and : and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mo Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
  note   = {Internet: https://arxiv.org/abs/2303.08774, Apr. 02, 2024},
  title  = {GPT-4 Technical Report}
}

@article{ouyang2022training,
  author = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
  note   = {Internet: https://arxiv.org/abs/2203.02155, Apr. 05, 2024},
  title  = {Training language models to follow instructions with human feedback}
}

@article{pan2023logiclm,
  author = {Liangming Pan and Alon Albalak and Xinyi Wang and William Yang Wang},
  note   = {Internet: https://arxiv.org/abs/2305.12295, Apr. 11, 2024},
  title  = {Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning}
}

@article{paul2023refiner,
  author = {Debjit Paul and Mete Ismayilzada and Maxime Peyrard and Beatriz Borges and Antoine Bosselut and Robert West and Boi Faltings},
  note   = {Internet: https://arxiv.org/abs/2304.01904, Apr. 27, 2024},
  title  = {REFINER: Reasoning Feedback on Intermediate Representations}
}

@article{peng2023check,
  author = {Baolin Peng and Michel Galley and Pengcheng He and Hao Cheng and Yujia Xie and Yu Hu and Qiuyuan Huang and Lars Liden and Zhou Yu and Weizhu Chen and Jianfeng Gao},
  note   = {Internet: https://arxiv.org/abs/2302.12813, Apr. 22, 2024},
  title  = {Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback}
}

@article{peng2023impact,
  author = {Sida Peng and Eirini Kalliamvakou and Peter Cihon and Mert Demirer},
  note   = {Internet: https://arxiv.org/abs/2302.06590, Apr. 13, 2024},
  title  = {The Impact of AI on Developer Productivity: Evidence from GitHub Copilot}
}

@inproceedings{pmlr-v202-lai23b,
  author    = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-Tau and Fried, Daniel and Wang, Sida and Yu, Tao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  month     = {23--29 Jul},
  pages     = {18319--18345},
  pdf       = {https://proceedings.mlr.press/v202/lai23b/lai23b.pdf},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  title     = {DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation},
  url       = {https://proceedings.mlr.press/v202/lai23b.html},
  volume    = {202},
  year      = {2023}
}

@article{press2021train,
  author = {Press, Ofir and Smith, Noah A and Lewis, Mike},
  note   = {Internet: https://arxiv.org/abs/2108.12409, Apr. 03, 2024},
  title  = {Train short, test long: Attention with linear biases enables input length extrapolation}
}

@article{radford2019language,
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  note   = {Internet: https://openai.com/index/better-language-models/, Apr. 03, 2024},
  title  = {Language models are unsupervised multitask learners}
}

@article{raffel2020exploring,
  author    = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal   = {The Journal of Machine Learning Research},
  number    = {1},
  pages     = {5485--5551},
  publisher = {JMLRORG},
  title     = {Exploring the limits of transfer learning with a unified text-to-text transformer},
  volume    = {21},
  year      = {2020}
}

@article{reimers2019sentence,
  author = {Reimers, Nils and Gurevych, Iryna},
  note   = {Internet: https://arxiv.org/abs/1908.10084, Apr. 04, 2024},
  title  = {Sentence-bert: Sentence embeddings using siamese bert-networks}
}

@article{ribeiro2023street,
  author = {Danilo Ribeiro and Shen Wang and Xiaofei Ma and Henry Zhu and Rui Dong and Deguang Kong and Juliette Burger and Anjelica Ramos and William Wang and Zhiheng Huang and George Karypis and Bing Xiang and Dan Roth},
  note   = {Internet: https://arxiv.org/abs/2302.06729, Apr. 18, 2024},
  title  = {STREET: A Multi-Task Structured Reasoning and Explanation Benchmark}
}

@inproceedings{robertson1994some,
  author       = {Robertson, Stephen E and Walker, Steve},
  booktitle    = {SIGIR’94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, organised by Dublin City University},
  organization = {Springer},
  pages        = {232--241},
  title        = {Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval},
  year         = {1994}
}

@article{saravia2022prompt,
  author = {Saravia, Elvis},
  note   = {Internet: https://github.com/dair-ai/Prompt-Engineering-Guide, Apr. 07, 2024},
  title  = {Prompt Engineering Guide}
}

@article{scheurer2023training,
  author = {Jérémy Scheurer and Jon Ander Campos and Tomasz Korbak and Jun Shern Chan and Angelica Chen and Kyunghyun Cho and Ethan Perez},
  note   = {Internet: https://arxiv.org/abs/2303.16755, Apr. 16, 2024},
  title  = {Training Language Models with Language Feedback at Scale}
}

@article{schick2022peer,
  author = {Timo Schick and Jane Dwivedi-Yu and Zhengbao Jiang and Fabio Petroni and Patrick Lewis and Gautier Izacard and Qingfei You and Christoforos Nalmpantis and Edouard Grave and Sebastian Riedel},
  note   = {Internet: https://arxiv.org/abs/2208.11663, Apr. 28, 2024},
  title  = {PEER: A Collaborative Language Model}
}

@article{schick2023toolformer,
  author = {Timo Schick and Jane Dwivedi-Yu and Roberto Dessì and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
  note   = {Internet: https://arxiv.org/abs/2302.04761, Apr. 25, 2024},
  title  = {Toolformer: Language Models Can Teach Themselves to Use Tools}
}

@article{schramowski2022large,
  author = {Patrick Schramowski and Cigdem Turan and Nico Andersen and Constantin A. Rothkopf and Kristian Kersting},
  note   = {Internet: https://arxiv.org/abs/2103.11790, Apr. 25, 2024},
  title  = {Large Pre-trained Language Models Contain Human-like Biases of What is Right and Wrong to Do}
}

@article{schulman2017proximal,
  author = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  note   = {Internet: https://arxiv.org/abs/1707.06347, Apr. 16, 2024},
  title  = {Proximal Policy Optimization Algorithms}
}

@article{selfee2023,
  author = {Ye, Seonghyeon and Jo, Yongrae and Kim, Doyoung and Kim, Sungdong and Hwang, Hyeonbin and Seo, Minjoon},
  note   = {Internet: https://lklab.kaist.ac.kr/SelFee/, Apr. 04, 2024},
  title  = {SelFee: Iterative Self-Revising LLM Empowered by Self-Feedback Generation}
}

@article{sellam-etal-2020-bleurt,
  author = {Sellam, Thibault and Das, Dipanjan and Parikh, Ankur P},
  note   = {Internet: https://arxiv.org/abs/2004.04696, Apr. 07, 2024},
  title  = {BLEURT: Learning robust metrics for text generation}
}

@article{Shahaf_Horvitz_2010,
  author  = {Shahaf, Dafna and Horvitz, Eric},
  doi     = {10.1609/aaai.v24i1.7652},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  month   = {Jul.},
  number  = {1},
  pages   = {986-993},
  title   = {Generalized Task Markets for Human and Machine Computation},
  url     = {https://ojs.aaai.org/index.php/AAAI/article/view/7652},
  volume  = {24},
  year    = {2010}
}

@article{shaikh-etal-2023-second,
  author = {Shaikh, Omar and Zhang, Hongxin and Held, William and Bernstein, Michael and Yang, Diyi},
  note   = {Internet: https://arxiv.org/abs/2212.08061, Apr. 06, 2024},
  title  = {On second thought, let's not think step by step! Bias and toxicity in zero-shot reasoning}
}

@article{shazeer2020glu,
  author = {Shazeer, Noam},
  note   = {Internet: https://arxiv.org/abs/2002.05202, Apr. 01, 2024},
  title  = {Glu variants improve transformer}
}

@article{shen-etal-2016-minimum,
  author = {Shen, Shiqi and Cheng, Yong and He, Zhongjun and He, Wei and Wu, Hua and Sun, Maosong and Liu, Yang},
  note   = {Internet: https://arxiv.org/abs/1512.02433, Apr. 23, 2024},
  title  = {Minimum risk training for neural machine translation}
}

@article{shinn2023reflexion,
  author = {Noah Shinn and Federico Cassano and Edward Berman and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
  note   = {Internet: https://arxiv.org/abs/2303.11366, Apr. 25, 2024},
  title  = {Reflexion: Language Agents with Verbal Reinforcement Learning}
}

@article{shleifer2021normformer,
  author = {Shleifer, Sam and Weston, Jason and Ott, Myle},
  note   = {Internet: https://arxiv.org/abs/2110.09456, Apr. 11, 2024},
  title  = {Normformer: Improved transformer pretraining with extra normalization}
}

@article{singla2015learning,
  author = {Adish Singla and Eric Horvitz and Pushmeet Kohli and Andreas Krause},
  note   = {Internet: https://arxiv.org/abs/1508.02823, Apr. 03, 2024},
  title  = {Learning to Hire Teams}
}

@article{solaiman2019release,
  author = {Irene Solaiman and Miles Brundage and Jack Clark and Amanda Askell and Ariel Herbert-Voss and Jeff Wu and Alec Radford and Gretchen Krueger and Jong Wook Kim and Sarah Kreps and Miles McCain and Alex Newhouse and Jason Blazakis and Kris McGuffie and Jasmine Wang},
  note   = {Internet: https://arxiv.org/abs/1908.09203, Apr. 01, 2024},
  title  = {Release Strategies and the Social Impacts of Language Models}
}

@article{srivastava2014dropout,
  author    = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal   = {The journal of machine learning research},
  number    = {1},
  pages     = {1929--1958},
  publisher = {JMLR. org},
  title     = {Dropout: a simple way to prevent neural networks from overfitting},
  volume    = {15},
  year      = {2014}
}

@article{su2024roformer,
  author    = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal   = {Neurocomputing},
  pages     = {127063},
  publisher = {Elsevier},
  title     = {Roformer: Enhanced transformer with rotary position embedding},
  volume    = {568},
  year      = {2024}
}

@article{sun2023principle,
  author = {Sun, Zhiqing and Shen, Yikang and Zhou, Qinhong and Zhang, Hongxin and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
  note   = {Internet: https://arxiv.org/abs/2305.03047, Apr. 17, 2024},
  title  = {Principle-driven self-alignment of language models from scratch with minimal human supervision}
}

@article{tafjord-etal-2022-entailer,
  author = {Tafjord, Oyvind and Mishra, Bhavana Dalvi and Clark, Peter},
  note   = {Internet: https://arxiv.org/abs/2210.12217, Apr. 18, 2024},
  title  = {Entailer: Answering questions with faithful and truthful chains of reasoning}
}

@article{tay2022unifying,
  author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},
  note   = {Internet: https://arxiv.org/abs/2205.05131, Apr. 04, 2024},
  title  = {Unifying language learning paradigms}
}

@article{thoppilan2022lamda,
  author = {Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and YaGuang Li and Hongrae Lee and Huaixiu Steven Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Vincent Zhao and Yanqi Zhou and Chung-Ching Chang and Igor Krivokon and Will Rusch and Marc Pickett and Pranesh Srinivasan and Laichee Man and Kathleen Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Soraker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark Diaz and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravi Rajakumar and Alena Butryna and Matthew Lamm and Viktoriya Kuzmina and Joe Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},
  note   = {Internet: https://arxiv.org/abs/2201.08239, Apr. 01, 2024},
  title  = {LaMDA: Language Models for Dialog Applications}
}

@article{touvron2023llama,
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  note   = {Internet: https://arxiv.org/abs/2307.09288, Apr. 25, 2024},
  title  = {Llama 2: Open foundation and fine-tuned chat models}
}

@article{uesato2022solving,
  author = {Jonathan Uesato and Nate Kushman and Ramana Kumar and Francis Song and Noah Siegel and Lisa Wang and Antonia Creswell and Geoffrey Irving and Irina Higgins},
  note   = {Internet: https://arxiv.org/abs/2211.14275, Apr. 05, 2024},
  title  = {Solving math word problems with process- and outcome-based feedback}
}

@article{varshney2023stitch,
  author = {Neeraj Varshney and Wenlin Yao and Hongming Zhang and Jianshu Chen and Dong Yu},
  note   = {Internet: https://arxiv.org/abs/2307.03987, Apr. 01, 2024},
  title  = {A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation}
}

@article{vaswani2017attention,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  note   = {Internet: https://arxiv.org/abs/1706.03762, Apr. 01, 2024},
  title  = {Attention is all you need}
}

@article{wang2022deepnet,
  author = {Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu},
  note   = {Internet: https://arxiv.org/abs/2203.00555, Apr. 04, 2024},
  title  = {Deepnet: Scaling transformers to 1,000 layers}
}

@inproceedings{wang2022language,
  author       = {Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Le Scao, Teven and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin},
  booktitle    = {International Conference on Machine Learning},
  organization = {PMLR},
  pages        = {22964--22984},
  title        = {What language model architecture and pretraining objective works best for zero-shot generalization?},
  year         = {2022}
}

@article{wang2022self,
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  note   = {Internet: https://arxiv.org/abs/2203.11171, Apr. 24, 2024},
  title  = {Self-consistency improves chain of thought reasoning in language models}
}

@article{wang2022super,
  author = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  note   = {Internet: https://arxiv.org/abs/2204.07705, Apr. 12, 2024},
  title  = {Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks}
}

@article{wang2023aligning,
  author = {Yufei Wang and Wanjun Zhong and Liangyou Li and Fei Mi and Xingshan Zeng and Wenyong Huang and Lifeng Shang and Xin Jiang and Qun Liu},
  note   = {Internet: https://arxiv.org/abs/2307.12966, Apr. 21, 2024},
  title  = {Aligning Large Language Models with Human: A Survey}
}

@article{wang2023selfconsistency,
  author = {Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
  note   = {Internet: https://arxiv.org/abs/2203.11171, Apr. 07, 2024},
  title  = {Self-Consistency Improves Chain of Thought Reasoning in Language Models}
}

@inproceedings{webster1992tokenization,
  author    = {Webster, Jonathan J and Kit, Chunyu},
  booktitle = {COLING 1992 volume 4: The 14th international conference on computational linguistics},
  title     = {Tokenization as the initial phase in NLP},
  year      = {1992}
}

@article{wei2022chain,
  author  = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal = {Advances in Neural Information Processing Systems},
  pages   = {24824--24837},
  title   = {Chain-of-thought prompting elicits reasoning in large language models},
  volume  = {35},
  year    = {2022}
}

@article{wei2023chainofthought,
  author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
  note   = {Internet: https://arxiv.org/abs/2201.11903, Apr. 04, 2024},
  title  = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}
}

@article{weidinger2021ethical,
  author = {Laura Weidinger and John Mellor and Maribeth Rauh and Conor Griffin and Jonathan Uesato and Po-Sen Huang and Myra Cheng and Mia Glaese and Borja Balle and Atoosa Kasirzadeh and Zac Kenton and Sasha Brown and Will Hawkins and Tom Stepleton and Courtney Biles and Abeba Birhane and Julia Haas and Laura Rimell and Lisa Anne Hendricks and William Isaac and Sean Legassick and Geoffrey Irving and Iason Gabriel},
  note   = {Internet: https://arxiv.org/abs/2112.04359, Apr. 18, 2024},
  title  = {Ethical and social risks of harm from Language Models}
}

@article{welleck2022generating,
  author = {Sean Welleck and Ximing Lu and Peter West and Faeze Brahman and Tianxiao Shen and Daniel Khashabi and Yejin Choi},
  note   = {Internet: https://arxiv.org/abs/2211.00053, Apr. 16, 2024},
  title  = {Generating Sequences by Learning to Self-Correct}
}

@article{weng2023large,
  author = {Yixuan Weng and Minjun Zhu and Fei Xia and Bin Li and Shizhu He and Shengping Liu and Bin Sun and Kang Liu and Jun Zhao},
  note   = {Internet: https://arxiv.org/abs/2212.09561, Apr. 08, 2024},
  title  = {Large Language Models are Better Reasoners with Self-Verification}
}

@article{wu2021textgail,
  author = {Qingyang Wu and Lei Li and Zhou Yu},
  note   = {Internet: https://arxiv.org/abs/2004.13796, Apr. 23, 2024},
  title  = {TextGAIL: Generative Adversarial Imitation Learning for Text Generation}
}

@article{xie2023selfevaluation,
  author = {Yuxi Xie and Kenji Kawaguchi and Yiran Zhao and Xu Zhao and Min-Yen Kan and Junxian He and Qizhe Xie},
  note   = {Internet: https://arxiv.org/abs/2305.00633, Apr. 23, 2024},
  title  = {Self-Evaluation Guided Beam Search for Reasoning}
}

@article{xu-etal-2022-errors,
  author = {Xu, Wenda and Tuan, Yilin and Lu, Yujie and Saxon, Michael and Li, Lei and Wang, William Yang},
  note   = {Internet: https://arxiv.org/abs/2210.05035, Apr. 11, 2024},
  title  = {Not all errors are equal: Learning text generation metrics using stratified error synthesis}
}

@article{xu-etal-2023-sescore2,
  author = {Xu, Wenda and Qian, Xian and Wang, Mingxuan and Li, Lei and Wang, William Yang},
  note   = {Internet: https://arxiv.org/abs/2212.09305, Apr. 07, 2024},
  title  = {SESCORE2: Learning text generation evaluation via synthesizing realistic mistakes}
}

@article{xue2020mt5,
  author = {Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  note   = {Internet: https://arxiv.org/abs/2010.11934, Apr. 25, 2024},
  title  = {mT5: A massively multilingual pre-trained text-to-text transformer}
}

@article{yan-etal-2023-bleurt,
  author = {Yan, Yiming and Wang, Tao and Zhao, Chengqi and Huang, Shujian and Chen, Jiajun and Wang, Mingxuan},
  note   = {Internet: https://arxiv.org/abs/2307.03131, Apr. 25, 2024},
  title  = {BLEURT has universal translations: An analysis of automatic metrics by minimum risk training}
}

@article{yan-etal-2023-learning,
  author = {Yan, Hao and Srivastava, Saurabh and Tai, Yintao and Wang, Sida I and Yih, Wen-tau and Yao, Ziyu},
  note   = {Internet: https://arxiv.org/abs/2305.08195, Apr. 19, 2024},
  title  = {Learning to simulate natural language feedback for interactive semantic parsing}
}

@article{yang-etal-2022-generating,
  author = {Yang, Kaiyu and Deng, Jia and Chen, Danqi},
  note   = {Internet: https://arxiv.org/abs/2205.12443, Apr. 22, 2024},
  title  = {Generating natural language proofs with verifier-guided search}
}

@article{yang-etal-2022-re3,
  author = {Yang, Kevin and Tian, Yuandong and Peng, Nanyun and Klein, Dan},
  note   = {Internet: https://arxiv.org/abs/2210.06774, Apr. 23, 2024},
  title  = {Re3: Generating longer stories with recursive reprompting and revision}
}

@article{yang-klein-2021-fudge,
  author = {Yang, Kevin and Klein, Dan},
  note   = {Internet: https://arxiv.org/abs/2104.05218, Apr. 23, 2024},
  title  = {FUDGE: Controlled text generation with future discriminators}
}

@article{yao2023tree,
  author = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
  note   = {Internet: https://arxiv.org/abs/2305.10601, Apr. 05, 2024},
  title  = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models}
}

@article{yu2023improving,
  author = {Wenhao Yu and Zhihan Zhang and Zhenwen Liang and Meng Jiang and Ashish Sabharwal},
  note   = {Internet: https://arxiv.org/abs/2305.14002, Apr. 30, 2024},
  title  = {Improving Language Models via Plug-and-Play Retrieval Feedback}
}

@article{zelikman2022star,
  author = {Eric Zelikman and Yuhuai Wu and Jesse Mu and Noah D. Goodman},
  note   = {Internet: https://arxiv.org/abs/2203.14465, Apr. 07, 2024},
  title  = {STaR: Bootstrapping Reasoning With Reasoning}
}

@article{zhang2019root,
  author  = {Zhang, Biao and Sennrich, Rico},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Root mean square layer normalization},
  volume  = {32},
  year    = {2019}
}

@article{zhang2020bertscore,
  author = {Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
  note   = {Internet: https://arxiv.org/abs/1904.09675, Apr. 14, 2024},
  title  = {BERTScore: Evaluating Text Generation with BERT}
}

@article{zhang2022automatic,
  author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  note   = {Internet: https://arxiv.org/abs/2210.03493, Apr. 08, 2024},
  title  = {Automatic chain of thought prompting in large language models}
}

@article{zhang2023algo,
  author = {Kexun Zhang and Danqing Wang and Jingtao Xia and William Yang Wang and Lei Li},
  note   = {Internet: https://arxiv.org/abs/2305.14591, Apr. 21, 2024},
  title  = {ALGO: Synthesizing Algorithmic Programs with LLM-Generated Oracle Verifiers}
}

@article{zhang2023language,
  author = {Muru Zhang and Ofir Press and William Merrill and Alisa Liu and Noah A. Smith},
  note   = {Internet: https://arxiv.org/abs/2305.13534, Apr. 25, 2024},
  title  = {How Language Model Hallucinations Can Snowball}
}

@article{zhang2023selfedit,
  author = {Kechi Zhang and Zhuo Li and Jia Li and Ge Li and Zhi Jin},
  note   = {Internet: https://arxiv.org/abs/2305.04087, Apr. 14, 2024},
  title  = {Self-Edit: Fault-Aware Code Editor for Code Generation}
}

@article{zhao2023survey,
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  note   = {Internet: https://arxiv.org/abs/2303.18223, Apr. 29, 2024},
  title  = {A survey of large language models}
}

@article{zhu-etal-2023-solving,
  author = {Zhu, Xinyu and Wang, Junjie and Zhang, Lin and Zhang, Yuxiang and Huang, Yongfeng and Gan, Ruyi and Zhang, Jiaxing and Yang, Yujiu},
  note   = {Internet: https://arxiv.org/abs/2210.16257, Apr. 04, 2024},
  title  = {Solving math word problems via cooperative reasoning induced language models}
}

@article{ziegler2019fine,
  author = {Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  note   = {Internet: https://arxiv.org/abs/1909.08593, Apr. 16, 2024},
  title  = {Fine-tuning language models from human preferences}
}
